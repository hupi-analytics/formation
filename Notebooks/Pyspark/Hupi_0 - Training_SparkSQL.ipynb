{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SparkSQL  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comment construire un DataFrame à partir de sources diverses?  \n",
    "\n",
    "How to create a DataFrame from different sources?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pyspark.context.SparkContext object at 0x7f6e70939cd0>\n"
     ]
    }
   ],
   "source": [
    "# \"numpy\" est une extension destinée à manipuler des matrices ou tableaux \n",
    "#    multidimensionnels ainsi que des fonctions mathématiques sur ces tableaux.\n",
    "# \"numpy\" is the fundamental package for scientific computing with Python. It\n",
    "#    contains a powerful N-dimensional array object, useful linear algebra.\n",
    "import numpy as np\n",
    "\n",
    "# \"math\" est une extension qui fournit des fonctions mathématiques définies par C standard.\n",
    "# Package math provides access to the mathematical functions defined by the C standard.\n",
    "import math\n",
    "\n",
    "# On vérifie qu'il existe un Spark Context \n",
    "# We test if Spark Context existed\n",
    "# from pyspark import SparkContext # Si il n'existe pas déjà / if it doesn't exist\n",
    "#sc = SparkContext(appName=\"PythonSparkContext\")\n",
    "print(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A partir de fichiers JSON - From JSON files to DataFrame "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Créer SQLContext -  Build SQLContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lire le fichier  - Read file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = sqlContext.read.json(\"hdfs:///user/ecoles/test_Helene/file.json\").cache()\n",
    "# data = sqlContext.read.json(\"hdfs://ecoles.node1.pro.hupi.loc/user/ecoles/test_Helene/file.json\").cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# \"data\" est un Dataframe\n",
    "# \"data\" is a Dataframe\n",
    "type(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(CP=64600, age=25, nom=u'Smith', prenom=u'John'),\n",
       " Row(CP=59000, age=24, nom=u'Kelly', prenom=u'Stuart'),\n",
       " Row(CP=64000, age=20, nom=u'Kerry', prenom=u'John'),\n",
       " Row(CP=49000, age=28, nom=u'Smith', prenom=u'Anne'),\n",
       " Row(CP=64600, age=22, nom=u'Leclerc', prenom=u'Henry')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(CP=64600, age=25, nom=u'Smith', prenom=u'John')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Selectionner la premiere ligne de data\n",
    "# Select first row of data\n",
    "data.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(CP=64600, age=25, nom=u'Smith', prenom=u'John'),\n",
       " Row(CP=59000, age=24, nom=u'Kelly', prenom=u'Stuart'),\n",
       " Row(CP=64000, age=20, nom=u'Kerry', prenom=u'John')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Selection des 3 premières lignes de data\n",
    "# Select the 3 first rows of data\n",
    "data.take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Afficher le dataframe - Display the content of the DataFrame  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+-------+------+\n",
      "|   CP|age|    nom|prenom|\n",
      "+-----+---+-------+------+\n",
      "|64600| 25|  Smith|  John|\n",
      "|59000| 24|  Kelly|Stuart|\n",
      "|64000| 20|  Kerry|  John|\n",
      "|49000| 28|  Smith|  Anne|\n",
      "|64600| 22|Leclerc| Henry|\n",
      "|75013| 26| Robert| Alice|\n",
      "|75004| 20| Dupont|Pierre|\n",
      "+-----+---+-------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Afficher les noms de colonnes - Print  the schema of column names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- CP: long (nullable = true)\n",
      " |-- age: long (nullable = true)\n",
      " |-- nom: string (nullable = true)\n",
      " |-- prenom: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Faire des requetes sur un dataframe - Run queries on DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Sélectionner la colonne \"nom\" \n",
    "# Select only the \"nom\" column\n",
    "name = data.select(\"nom\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Le résultat est - The result is \n",
      "+-------+\n",
      "|    nom|\n",
      "+-------+\n",
      "|  Smith|\n",
      "|  Kelly|\n",
      "|  Kerry|\n",
      "|  Smith|\n",
      "|Leclerc|\n",
      "| Robert|\n",
      "| Dupont|\n",
      "+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Le résultat est - The result is \")\n",
    "name.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Sélectionner deux colonnes \"nom\" et \"prenom\"\n",
    "# Select two columns \"nom\" and \"prenom\"\n",
    "fullname = data.select(data['nom'], data['prenom'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Le résultat est - The result is \n",
      "+-------+------+\n",
      "|    nom|prenom|\n",
      "+-------+------+\n",
      "|  Smith|  John|\n",
      "|  Kelly|Stuart|\n",
      "|  Kerry|  John|\n",
      "|  Smith|  Anne|\n",
      "|Leclerc| Henry|\n",
      "| Robert| Alice|\n",
      "| Dupont|Pierre|\n",
      "+-------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Le résultat est - The result is \")\n",
    "fullname.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Sélectionner les personnes ayant plus de 22 ans\n",
    "# Select people who is older than 22 years old\n",
    "val = data.filter(data['age'] > 22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Le résultat est - The result is \n",
      "+-----+---+------+------+\n",
      "|   CP|age|   nom|prenom|\n",
      "+-----+---+------+------+\n",
      "|64600| 25| Smith|  John|\n",
      "|59000| 24| Kelly|Stuart|\n",
      "|49000| 28| Smith|  Anne|\n",
      "|75013| 26|Robert| Alice|\n",
      "+-----+---+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Le résultat est - The result is \")\n",
    "val.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Pour faire des requètes SQL il faut passer par cette étape \n",
    "# To run queries SQL, we must follow this step\n",
    "data.registerTempTable(\"data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Sélectionner la colonne \"CP\" \n",
    "# Select column \"CP\" \n",
    "cp = sqlContext.sql(\"select CP from data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(CP=64600),\n",
       " Row(CP=59000),\n",
       " Row(CP=64000),\n",
       " Row(CP=49000),\n",
       " Row(CP=64600),\n",
       " Row(CP=75013),\n",
       " Row(CP=75004)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cp.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A partir de csv, txt - From csv, txt to DataFrame "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* La lecture d'un fichier csv ou txt est un peu différente que celle d'un JSON car il faut bien préciser le nom des colonnes du fichier texte.\n",
    "\n",
    "\n",
    "* Read a text file is a little different from JSON file because we need to determine columns names in these file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Importer SQLContext et types de data - Import SQLContext and data types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext, Row"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create SQLContext - Build SQLContext "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lire le fichier et convertir chaque phrase en  tuple - Read text file and convert each line to a tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lines = sc.textFile(\"hdfs:///user/ecoles/test_Helene/file1.txt\",use_unicode=False).cache()\n",
    "parts = lines.map(lambda l: l.split(\",\"))\n",
    "base = parts.map(lambda p : Row(CP=p[0], ville=p[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.rdd.PipelinedRDD"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# \"base\" est un RDD ( PipelinesRDD est un RDD avec des étapes à effectuer (map) )\n",
    "# \"base\" is a RDD ( Pipeline is a RDD with some jobs to do on it (map) )\n",
    "type(base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(CP='44000', ville=' Nantes'),\n",
       " Row(CP='49000', ville=' Angers'),\n",
       " Row(CP='50000', ville=' Baudre')]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base.take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Il nous reste à convertir un RDD en DataFrame\n",
    "\n",
    "* It remains to us to convert a RDD to a DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Créer un DataFrame - Create DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "schemaData = sqlContext.createDataFrame(base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# On a schemaData qui est un DataFrame\n",
    "# We have schemaData as a DataFrame\n",
    "type(schemaData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- CP: string (nullable = true)\n",
      " |-- ville: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "schemaData.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Pour faire des requetes SQL il faut passer à cette étape \n",
    "# To run queries SQL, we must follow this step\n",
    "schemaData.registerTempTable(\"base\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Maintenant, on pourra faire des requetes SQL sur \"base\"\n",
    "\n",
    "* Now, we can execute SQL queries on \"base\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Faire des requêtes sur un dataframe - Run queries on DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dat = sqlContext.sql(\"SELECT * FROM base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# le resultat quand on applique requete SQl sur un DataFrame est aussi un DataFrame\n",
    "# By running SQL queries on DataFrame, we will also have a DataFrame\n",
    "type(dat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------------+\n",
      "|   CP|             ville|\n",
      "+-----+------------------+\n",
      "|44000|            Nantes|\n",
      "|49000|            Angers|\n",
      "|50000|            Baudre|\n",
      "|51100|             Reims|\n",
      "|59000|             Lille|\n",
      "|64500| Saint-Jean de Luz|\n",
      "|64600|            Anglet|\n",
      "|75013|          Paris 13|\n",
      "|75004|           Paris 4|\n",
      "+-----+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dat.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[CP: bigint, age: bigint, nom: string, prenom: string]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[47] at RDD at PythonRDD.scala:43"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jointure entre un fichier JSON et un fichier txt - Join a JSON file and a txt file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* On peut faire la jointure entre le fichier \"file.json\" et le fichier \"file1.txt\" pour retrouver le nom de ville en convertissant les deux fichiers en DataFrame puis on fait la jointure.\n",
    "\n",
    "\n",
    "* We can find city's names by converting two files in DataFrame, then do a join.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[ville: string]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sqlContext.sql(\"select ville from base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[CP: bigint, age: bigint, nom: string, prenom: string]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sqlContext.sql(\"select * from data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "table = sqlContext.sql(\"SELECT data.nom, data.prenom, data.age, base.ville FROM data JOIN base ON data.CP = base.CP\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(nom=u'Smith', prenom=u'Anne', age=28, ville=u' Angers'),\n",
       " Row(nom=u'Kelly', prenom=u'Stuart', age=24, ville=u' Lille'),\n",
       " Row(nom=u'Smith', prenom=u'John', age=25, ville=u' Anglet'),\n",
       " Row(nom=u'Leclerc', prenom=u'Henry', age=22, ville=u' Anglet'),\n",
       " Row(nom=u'Robert', prenom=u'Alice', age=26, ville=u' Paris 13'),\n",
       " Row(nom=u'Dupont', prenom=u'Pierre', age=20, ville=u' Paris 4')]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataFrame to RDD "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Pour convertir un DataFrame en RDD, on fait un \"map\".\n",
    "\n",
    "\n",
    "* To transform a DataFrame to a RDD, we must use function \"map\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Selectionner premiere colonne dans \"data\" et la convertir en RDD \n",
    "# Select first column in \"data\" and transform it to a RDD\n",
    "rdd = dat.map(lambda x : (x[0], x[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.rdd.PipelinedRDD"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(rdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'44000', u' Nantes'),\n",
       " (u'49000', u' Angers'),\n",
       " (u'50000', u' Baudre'),\n",
       " (u'51100', u' Reims'),\n",
       " (u'59000', u' Lille'),\n",
       " (u'64500', u' Saint-Jean de Luz'),\n",
       " (u'64600', u' Anglet'),\n",
       " (u'75013', u' Paris 13'),\n",
       " (u'75004', u' Paris 4')]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(rdd.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('44000', ' Nantes'),\n",
       " ('49000', ' Angers'),\n",
       " ('50000', ' Baudre'),\n",
       " ('51100', ' Reims'),\n",
       " ('59000', ' Lille'),\n",
       " ('64500', ' Saint-Jean de Luz'),\n",
       " ('64600', ' Anglet'),\n",
       " ('75013', ' Paris 13'),\n",
       " ('75004', ' Paris 4')]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.map( lambda l : (l[0].encode(\"utf8\") , l[1].encode(\"utf8\")) ) \\\n",
    "   .collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark 1G ",
   "language": "python",
   "name": "pyspark1g"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
