{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "________________________________________________________________________________________________________________________________\n",
    "\n",
    "________________________________________________________________________________________________________________________________\n",
    "\n",
    "\n",
    "# Analyse de sentiments en temps réel - Streaming sentiment analysis\n",
    "\n",
    "\n",
    "________________________________________________________________________________________________________________________________\n",
    "\n",
    "________________________________________________________________________________________________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sujet - Subject"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous allons récupérer les textes que vous allez taper directement sur vos ordinateurs et nous dirons à l'aide d'un algorithme statistique si votre texte est plutôt positif ou négatif. Le modèle utilisé est celui créé dans le notebook précédent.\n",
    "> \n",
    "*\n",
    "We are going to collect the text that you have taped on your computer and we will predict the sentiment (positive/negative) of this text. The model is the one which we have built in the previous notebook.\n",
    "*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Si il n'y a pas de SparkContext, il faut le créer / Create a SparkContext if there is not :\n",
    "#from pyspark import SparkContext # Si il n'existe pas déjà / if it doesn't exist\n",
    "#sc = SparkContext(appName=\"PythonStreamingKafka_SentimentAnalysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importation des libraries - Library import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Modules pour la liaison entre Kafka et Spark / Packages for linking Kafka et Spark\n",
    "\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.streaming.kafka import KafkaUtils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Modules pour le traitement de texte et le modèle / Packages for text formatting and model\n",
    "\n",
    "# Transformation des données / Data transformation\n",
    "from pyspark.mllib.linalg import Vectors  \n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "\n",
    "# Pour analyser les résultats / For analyzing the results\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "from pyspark.mllib.evaluation import BinaryClassificationMetrics\n",
    "\n",
    "# Pour la régression logistique / For logistic regression\n",
    "from pyspark.mllib.classification import LogisticRegressionWithLBFGS\n",
    "from pyspark.mllib.classification import LogisticRegressionModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Chargement du dictionnaire - Loading of the dictionnary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous disposons d'un dictionnaire de mots dont chaque mot est une variable du modèle.\n",
    "> \n",
    "*We have a dictionnay of words for which each word is a variable (a feature) of the model.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de mots retenus : 2668\n",
      "Type d'objet : <type 'list'>\n",
      "Apercu :\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('terriblement', 0),\n",
       " ('terribles', 0),\n",
       " ('territoire', 0),\n",
       " ('tete', 0),\n",
       " ('teule', 0),\n",
       " ('texte', 0),\n",
       " ('textes', 0),\n",
       " ('thanatonautes', 0),\n",
       " ('the', 0),\n",
       " ('theatre', 0)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "liste_mots = sc.textFile(\"hdfs://ecoles.node1.pro.hupi.loc/user/anthony.laffond/my_liste2600py/*\") \\\n",
    "               .map(lambda l: (l.encode(\"utf-8\"),0)).collect()\n",
    "\n",
    "print(\"Nombre de mots retenus : %d\" %len(liste_mots))\n",
    "print(\"Type d'objet : \"+str(type(liste_mots)))\n",
    "\n",
    "print(\"Apercu :\")\n",
    "pos = liste_mots.index((\"texte\",0))\n",
    "liste_mots[(pos-5):(pos+5)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mise en forme du texte - Text formatting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous devons d'abord nettoyer le texte des accents, caractères spéciaux et autres.\n",
    "> \n",
    "*First of all, we have to clean the text from any accents, specials characters and others.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['exemple', 'j', 'ai', 'reussi', 'a', 'nettoyer', 'ce', 'texte', 'et', 're', 'nettoyer', 'mon', 'texte']\n"
     ]
    }
   ],
   "source": [
    "def nettoyage_texte(text) :\n",
    "    texte = text.replace('À', 'A').replace('Á', 'A').replace('Â', 'A').replace('Ã', 'A') \\\n",
    "    .replace('È', 'E').replace('É', 'E').replace('Ê', 'E').replace('Ë', 'E') \\\n",
    "    .replace('Í', 'I').replace('Ì', 'I').replace('Î', 'I').replace('Ï', 'I') \\\n",
    "    .replace('Ù', 'U').replace('Ú', 'U').replace('Û', 'U').replace('Ü', 'U') \\\n",
    "    .replace('Ò', 'O').replace('Ó', 'O').replace('Ô', 'O').replace('Õ', 'O') \\\n",
    "    .replace('Ö', 'O').replace('Ñ', 'N').replace('Ç', 'C').replace('ª', 'A') \\\n",
    "    .replace('º', 'O').replace('§', 'S').replace('³', '3').replace('²', '2') \\\n",
    "    .replace('¹', '1').replace('à', 'a').replace('á', 'a').replace('â', 'a') \\\n",
    "    .replace('ã', 'a').replace('ä', 'a').replace('è', 'e').replace('é', 'e') \\\n",
    "    .replace('ê', 'e').replace('ë', 'e').replace('í', 'i').replace('ì', 'i') \\\n",
    "    .replace('î', 'i').replace('ï', 'i').replace('ù', 'u').replace('ú', 'u') \\\n",
    "    .replace('û', 'u').replace('ü', 'u').replace('ò', 'o').replace('ó', 'o') \\\n",
    "    .replace('ô', 'o').replace('õ', 'o').replace('ö', 'o').replace('ñ', 'n') \\\n",
    "    .replace('Ä', 'A').replace('ç', 'c') \\\n",
    "    .replace(\"!\",\" \").replace(\".\",\" \").replace(\"?\",\" \").replace(\",\",\" \") \\\n",
    "    .replace(\";\",\" \").replace(\":\",\" \").replace(\"/\",\" \").replace(\"+\",\" \") \\\n",
    "    .replace(\"%\",\" \").replace(\"(\",\" \").replace(\")\",\" \").replace(\"[\",\" \") \\\n",
    "    .replace(\"]\",\" \").replace(\"&\",\" \").replace(\"`\",\" \").replace(\"*\",\" \") \\\n",
    "    .replace(\"$\",\" \").replace(\"«\",\" \").replace(\"»\",\" \").replace(\"'\",\" \") \\\n",
    "    .replace(\"_\",\" \").replace(\"\\t\",\" \").replace(\"|\",\" \").replace(\"\\\"\",\" \") \\\n",
    "    .replace(\"0\",\" \").replace(\"1\",\" \").replace(\"2\",\" \").replace(\"3\",\" \") \\\n",
    "    .replace(\"4\",\" \").replace(\"5\",\" \").replace(\"6\",\" \").replace(\"7\",\" \") \\\n",
    "    .replace(\"8\",\" \").replace(\"9\",\" \") \\\n",
    "    .replace(\"!\",\" \").replace(\".\",\" \").replace(\"?\",\" \").replace(\",\",\" \") \\\n",
    "    .replace(\";\",\" \").replace(\":\",\" \").replace(\"/\",\" \").replace(\"+\",\" \") \\\n",
    "    .replace(\"%\",\" \").replace(\"(\",\" \").replace(\")\",\" \").replace(\"[\",\" \") \\\n",
    "    .replace(\"]\",\" \").replace(\"&\",\" \").replace(\"`\",\" \").replace(\"*\",\" \") \\\n",
    "    .replace(\"$\",\" \").replace(\"«\",\" \").replace(\"»\",\" \").replace(\"'\",\" \") \\\n",
    "    .replace(\"_\",\" \").replace(\"\\t\",\" \").replace(\"|\",\" \").replace(\"\\\"\",\" \") \\\n",
    "    .replace(\" -\",\" \").replace(\"- \",\" \").replace(\"--\",\" \").replace(\" - \",\" \") \\\n",
    "    .lower().strip().split()\n",
    "    return texte\n",
    "\n",
    "texte = nettoyage_texte(\"Ëxémplè : J'äï$ réùssî à néttöyer- ce texte et re- néttöyer môn 1 téxtè !\")\n",
    "print(texte)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensuite, il faut mettre en forme le texte pour que le modèle puisse lire les données\n",
    "> *Then, we have to format the text so that the model can read the data*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment attribué (par défaut) : 1.0\n",
      "Variables (mots) : \n",
      "[ 0.  0.  0.  0.  0.  2.  0.  0.  0.  0.]\n"
     ]
    }
   ],
   "source": [
    "def mise_en_forme(liste_mots, texte, sentiment = 1):\n",
    "    data = zip(texte, [texte.count(w) for w in texte]) # Contage des mots du texte (effectifs)\n",
    "    data = list(set(data))  # Retrait des doublons\n",
    "    data = [w for w in data if (w[0],0) in liste_mots] # Filtre les mots retenus précédemment (dans liste_mots)\n",
    "    # Ajout des mots retenus qui ne sont pas présents dans la phrase sous la forme: (mot,0)\n",
    "    data = data +[ w for w in liste_mots if w[0] not in [x[0] for x in data] ]\n",
    "    data = sorted(data, key=lambda l: l[0])  # On trie par ordre alphabétique des mots\n",
    "    data = LabeledPoint( sentiment ,Vectors.dense([w[1] for w in data]) )\n",
    "    return data\n",
    "final = mise_en_forme(liste_mots,texte)\n",
    "print(\"Sentiment attribué (par défaut) : \"+ str(final.label))\n",
    "print(\"Variables (mots) : \")\n",
    "print(final.features[(pos-5):(pos+5)])\n",
    "\n",
    "# Dans cet exemple, nous avons : / In this example, we have :\n",
    "# [ 0.  0.  0.  0.  0.  2.  0.  0.  0.  0.] \n",
    "# Le 2 correspond au mot \"texte\" qui apparait 2 fois dans la phrase / the \"2\" is the number of occurrence of \"texte\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chargement du modèle - Loading the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = LogisticRegressionModel.load(sc, \"hdfs://ecoles.node1.pro.hupi.loc/user/anthony.laffond/model_reglog2600py\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Fonction de conversion du score / Conversion of score function\n",
    "def my_pred(score):\n",
    "    if(score>=0.8):\n",
    "        val = u\"Positif\"\n",
    "    elif(score>=0.6):\n",
    "        val = u\"Moyennement positif\"\n",
    "    elif(score>=0.4):\n",
    "        val = u\"Neutre\"\n",
    "    elif(score>=0.2):\n",
    "        val = u\"Moyennement negatif\"\n",
    "    else:\n",
    "        val = u\"Negatif\"\n",
    "    return val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['exemple', 'j', 'ai', 'reussi', 'a', 'nettoyer', 'ce', 'texte', 'et', 're', 'nettoyer', 'mon', 'texte']\n",
      "Score attribué : 1\n",
      "Sentiment attribué : Positif\n"
     ]
    }
   ],
   "source": [
    "print(texte)\n",
    "score = model.predict(final.features)\n",
    "print(\"Score attribué : \" + str(score))\n",
    "print(\"Sentiment attribué : \" + str(my_pred(score)) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ëxémplè : J'äï$ réùssî à Faîrë de l'ànalise de SêntIment !\n",
      "Score prédit : 1\n",
      "Sentiment prédit : Positif\n"
     ]
    }
   ],
   "source": [
    "def sentiment_analysis(texte,liste_mots,model):\n",
    "    texte = nettoyage_texte(texte)\n",
    "    texte = mise_en_forme(liste_mots, texte)\n",
    "    pred = model.predict(texte.features)\n",
    "    return pred\n",
    "\n",
    "phrase = \"Ëxémplè : J'äï$ réùssî à Faîrë de l'ànalise de SêntIment !\"\n",
    "print(phrase)\n",
    "print(\"Score prédit : \" + str(sentiment_analysis(phrase,liste_mots,model) ) )\n",
    "print(\"Sentiment prédit : \" + my_pred(sentiment_analysis(phrase,liste_mots,model) ).encode(\"utf8\") )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Liaison avec Kafka - Link with Kafka"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialisation - Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "intervalle = 10 # Fenêtre de x secondes / x seconds window\n",
    "\n",
    "ssc = StreamingContext(sc, intervalle)\n",
    "\n",
    "zkQuorum = \"ecoles.node1.pro.hupi.loc:2181\"\n",
    "\n",
    "topic = {\"ecoles_anthony\": 1}\n",
    "\n",
    "identifiant = \"Anthony\"  # A modifier pour chaque personne / modify for each user\n",
    "\n",
    "streamdata = KafkaUtils.createStream(ssc, zkQuorum, identifiant, topic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Traitement - Treatment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Sauvegarder les messages en .txt / Save messages as .txt files \n",
    "# fichier = streamdata.map(lambda l: l[1])\n",
    "# fichier.saveAsTextFiles(\"hdfs://ecoles.node1.pro.hupi.loc/user/anthony.laffond/Streaming/fichier\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Fonction d'affichage / Display fonction\n",
    "def get_output(rdd):\n",
    "    li = rdd.collect()\n",
    "    for x in li:\n",
    "        print(str(x[0].encode(\"utf8\")) + \" --> Score = \" + str(x[1]) + \" --> \" + str(x[2]))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "intro = streamdata.count().map(lambda l : \"\\n Nombre de textes : \"+str(l))\n",
    "intro.pprint()\n",
    "\n",
    "fichier = streamdata.map(lambda l: l[1]) \\\n",
    "                    .map(lambda rdd : rdd[(rdd.find(\"\\\"log\\\":\")+len(\"\\\"log\\\":\")):rdd.find(\",\\\"action\\\"\")]) \\\n",
    "                    .map(lambda l : ( l , \\\n",
    "                                (sentiment_analysis(l.encode(\"utf8\"),liste_mots,model)*1.0), \\\n",
    "                                str(my_pred(sentiment_analysis(l.encode(\"utf8\"),liste_mots,model))) \\\n",
    "                                    ) )\n",
    "fichier.foreachRDD(lambda w : get_output(w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lancement du programme - Program's launching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2016-05-27 17:52:50\n",
      "-------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2016-05-27 17:53:00\n",
      "-------------------------------------------\n",
      "\n",
      " Nombre de textes : 1\n",
      "\n",
      "\"l'INSA est une école géniale !\" --> Score = 1.0 --> Positif\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ssc.start()\n",
    "ssc.awaitTermination(intervalle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ssc.stop(stopSparkContext=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark 3G",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
