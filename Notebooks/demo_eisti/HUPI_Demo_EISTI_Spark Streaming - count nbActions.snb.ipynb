{"metadata":{"id":"6ebb3b93-710c-4608-8c9a-8c35b4e168e4","name":"Spark Streaming - count nbActions","user_save_timestamp":"2018-01-29T15:49:19.894Z","auto_save_timestamp":"1970-01-01T00:00:00.000Z","language_info":{"name":"scala","file_extension":"scala","codemirror_mode":"text/x-scala"},"trusted":true,"sparkNotebook":null,"customLocalRepo":null,"customRepos":null,"customDeps":["org.apache.spark % spark-streaming-kafka-0-8_2.11 % 2.2.0"],"customImports":null,"customArgs":null,"customSparkConf":null,"customVars":null},"cells":[{"metadata":{"id":"A47A0BCADB5B468E8A3C71B74CB86E2A"},"cell_type":"markdown","source":"### Spark Streaming\n\nSpark Streaming is an extension of the core Spark API that enables scalable, high-throughput, \nfault-tolerant stream processing of live data streams. Data can be ingested from many sources like \nKafka, Flume, Kinesis, or TCP sockets, and can be processed using complex algorithms expressed \nwith high-level functions like map, reduce, join and window. Finally, processed data can be \npushed out to filesystems, databases, and live dashboards. In fact, you can apply Spark’s machine learning and graph processing algorithms on data streams."},{"metadata":{"id":"C5CFCAAC9A21424A83EB2A2A45EE3BF2"},"cell_type":"markdown","source":"First, we import the names of the Spark Streaming classes and some implicit conversions from StreamingContext into our environment in order to add useful methods to other classes we need (like DStream). StreamingContext is the main entry point for all streaming functionality. We create a local StreamingContext with two execution threads, and a batch interval of 1 second."},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":false,"id":"C9BB1D25CB1946838F4A2067B4187257"},"cell_type":"code","source":["import org.apache.spark.streaming._\n","import org.apache.spark.streaming.StreamingContext._\n","import org.apache.spark.streaming.kafka._\n","import org.apache.spark.SparkConf"],"outputs":[{"name":"stdout","output_type":"stream","text":"import org.apache.spark.streaming._\nimport org.apache.spark.streaming.StreamingContext._\nimport org.apache.spark.streaming.kafka._\nimport org.apache.spark.SparkConf\n"},{"metadata":{},"data":{"text/html":""},"output_type":"execute_result","execution_count":1,"time":"Took: 1.383s, at 2018-01-29 16:57"}]},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":false,"id":"A8C0CE835D564BF686EDBFB0F10CA91E"},"cell_type":"code","source":["//val sparkConf = new SparkConf().setAppName(\"DirectKafkaWordCount\")\n","val sparkConf = new SparkConf().setAppName(\"Test Kafka2\") //sparkContext.getConf\n","val intervalle = 10 // Fenêtre de x secondes / x seconds window\n","\n","val ssc =  new StreamingContext(sc, Seconds(intervalle)) \n","//ssc.checkpoint(\"checkpoint\")\n","//val Array(zkQuorum, group, topics, numThreads) = args*/\n","val zkQuorum = \"ecoles.node1.pro.hupi.loc\"\n","val group = \"DEMO_HUPI_VINCENT\"\n","val topics = \"ecoles_hupilytics_scandivie\"\n","val numThreads = \"1\""],"outputs":[{"name":"stdout","output_type":"stream","text":"sparkConf: org.apache.spark.SparkConf = org.apache.spark.SparkConf@2cf00f95\nintervalle: Int = 10\nssc: org.apache.spark.streaming.StreamingContext = org.apache.spark.streaming.StreamingContext@340b41d0\nzkQuorum: String = ecoles.node1.pro.hupi.loc\ngroup: String = DEMO_HUPI_VINCENT\ntopics: String = ecoles_hupilytics_scandivie\nnumThreads: String = 1\n"},{"metadata":{},"data":{"text/html":""},"output_type":"execute_result","execution_count":2,"time":"Took: 1.270s, at 2018-01-29 16:57"}]},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":false,"id":"D8CD5B3124FF4380AACC1818A6548610"},"cell_type":"code","source":["val topicMap = topics.split(\",\").map((_, numThreads.toInt)).toMap\n","val streamdata = KafkaUtils.createStream(ssc, zkQuorum, group, topicMap)"],"outputs":[{"name":"stdout","output_type":"stream","text":"topicMap: scala.collection.immutable.Map[String,Int] = Map(ecoles_hupilytics_scandivie -> 1)\nstreamdata: org.apache.spark.streaming.dstream.ReceiverInputDStream[(String, String)] = org.apache.spark.streaming.kafka.KafkaInputDStream@7dd704e7\n"},{"metadata":{},"data":{"text/html":""},"output_type":"execute_result","execution_count":3,"time":"Took: 1.663s, at 2018-01-29 16:57"}]},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":false,"id":"0EE2F749074C4BA09515259EAA1B1D41"},"cell_type":"code","source":["// Fonction d'affichage de résultat\n","def get_output(rdd: RDD[String]) = {\n","  val li = rdd.collect()\n","  for(x <- li){\n","    println(x + \" \")\n","  }\n","  println(\"\")\n","  println(\"\")\n","} "],"outputs":[{"name":"stdout","output_type":"stream","text":"get_output: (rdd: org.apache.spark.rdd.RDD[String])Unit\n"},{"metadata":{},"data":{"text/html":""},"output_type":"execute_result","execution_count":4,"time":"Took: 1.063s, at 2018-01-29 16:57"}]},{"metadata":{"id":"211B8763F20F43EC829C95AB8D913B1D"},"cell_type":"markdown","source":"### A choisir un output désiré, par exemple, ici si on veut imprimer les messages, il faut commenter nbActions et vice versa"},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":false,"id":"2403A194F72647DCBC466F08498ECEDE"},"cell_type":"code","source":["/*\n","// Affichage des messages / Display messages\n","val message = streamdata.map(_._2)\n","message.foreachRDD(l=>get_output(l))\n","*/\n","\n","// ou Nombre d'actions par intervalle de temps\n","val nbactions = streamdata.count().map(l => \" - Nombre d'actions sur le site : \"+l.toString)\n","nbactions.foreachRDD(l => get_output(l))"],"outputs":[{"name":"stdout","output_type":"stream","text":"message: org.apache.spark.streaming.dstream.DStream[String] = org.apache.spark.streaming.dstream.MappedDStream@60980b0f\n"},{"metadata":{},"data":{"text/html":""},"output_type":"execute_result","execution_count":5,"time":"Took: 1.373s, at 2018-01-29 16:55"}]},{"metadata":{"id":"D3E8727BAAD54934ACEF83C6DD50CF45"},"cell_type":"markdown","source":"## Lancer Spark Streaming"},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":false,"id":"E0C7DA6BD2C5425C86FC64F06631F04F"},"cell_type":"code","source":["ssc.start()\n","val duration = 6*intervalle*1000.toLong  // 1 minute // on renvoie les counts dans 1 minute\n","ssc.awaitTerminationOrTimeout(duration)"],"outputs":[{"name":"stdout","output_type":"stream","text":" - Nombre d'actions sur le site : 0 \n\n\n - Nombre d'actions sur le site : 0 \n\n\n - Nombre d'actions sur le site : 0 \n\n\n - Nombre d'actions sur le site : 0 \n\n\n - Nombre d'actions sur le site : 0 \n\n\n - Nombre d'actions sur le site : 1 \n\n\nduration: Long = 60000\nres7: Boolean = false\n"},{"metadata":{},"data":{"text/html":"false"},"output_type":"execute_result","execution_count":6,"time":"Took: 1m1.395s, at 2018-01-29 16:58"}]},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":true,"id":"31A7DD8D69394A849A9E603C1CF9146B"},"cell_type":"code","source":[""],"outputs":[]}],"nbformat":4}