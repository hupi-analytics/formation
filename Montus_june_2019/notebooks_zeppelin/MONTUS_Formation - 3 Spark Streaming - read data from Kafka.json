{"paragraphs":[{"text":"%md\nThis notebook aims to discover how to use Spark Streaming. \n\n\"Spark Streaming is an extension of the core Spark API that enables scalable, high-throughput, fault-tolerant stream processing of live data streams. Data can be ingested from many sources like Kafka, Flume, Kinesis, or TCP sockets, and can be processed using complex algorithms expressed with high-level functions like map, reduce, join and window. Finally, processed data can be pushed out to filesystems, databases, and live dashboards. In fact, you can apply Spark’s machine learning and graph processing algorithms on data streams.\" (source : spark.apache.org)\n\n***Reference link***\nhttps://spark.apache.org/docs/2.2.0/streaming-programming-guide.html\nhttps://spark.apache.org/docs/2.2.0/structured-streaming-kafka-integration.html","user":"anonymous","dateUpdated":"2019-06-19T09:09:59+0000","config":{"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"results":{},"enabled":true,"editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>This notebook aims to discover how to use Spark Streaming. </p>\n<p>&ldquo;Spark Streaming is an extension of the core Spark API that enables scalable, high-throughput, fault-tolerant stream processing of live data streams. Data can be ingested from many sources like Kafka, Flume, Kinesis, or TCP sockets, and can be processed using complex algorithms expressed with high-level functions like map, reduce, join and window. Finally, processed data can be pushed out to filesystems, databases, and live dashboards. In fact, you can apply Spark’s machine learning and graph processing algorithms on data streams.&rdquo; (source : spark.apache.org)</p>\n<p><strong><em>Reference link</em></strong><br/><a href=\"https://spark.apache.org/docs/2.2.0/streaming-programming-guide.html\">https://spark.apache.org/docs/2.2.0/streaming-programming-guide.html</a><br/><a href=\"https://spark.apache.org/docs/2.2.0/structured-streaming-kafka-integration.html\">https://spark.apache.org/docs/2.2.0/structured-streaming-kafka-integration.html</a></p>\n</div>"}]},"apps":[],"jobName":"paragraph_1560422217622_-1355940464","id":"20190610-090014_1338988644","dateCreated":"2019-06-13T10:36:57+0000","dateStarted":"2019-06-19T09:09:59+0000","dateFinished":"2019-06-19T09:09:59+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:19743"},{"text":"%md\n# Import libraries\n\nWe need to import library for spark streaming kafka and kafka. Be careful of the version of Spark using. Here, we use Spark 2.2.1. So we need to find the library corresponding for our Spark version.","user":"anonymous","dateUpdated":"2019-06-18T07:27:00+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>Import libraries</h1>\n<p>We need to import library for spark streaming kafka and kafka. Be careful of the version of Spark using. Here, we use Spark 2.2.1. So we need to find the library corresponding for our Spark version.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1560516987534_1434925125","id":"20190614-125627_774768012","dateCreated":"2019-06-14T12:56:27+0000","dateStarted":"2019-06-18T07:27:00+0000","dateFinished":"2019-06-18T07:27:00+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:19744"},{"text":"%spark.dep\nz.load(\"/opt/hupi/JAR/spark-streaming-kafka-0-8_2.11-2.2.1.jar\")\nz.load(\"/opt/hupi/JAR/kafka_2.10-0.10.2.1.jar\")\nz.load(\"/opt/hupi/JAR/spark-sql-kafka-0-10_2.11-2.2.1.jar\")\nz.load(\"/opt/hupi/JAR/kafka-clients-0.10.1.0.jar\")","user":"anonymous","dateUpdated":"2019-06-19T09:52:48+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"res1: org.apache.zeppelin.dep.Dependency = org.apache.zeppelin.dep.Dependency@6b9a2077\n"}]},"apps":[],"jobName":"paragraph_1560517405511_142844311","id":"20190614-130325_2086890444","dateCreated":"2019-06-14T13:03:25+0000","dateStarted":"2019-06-19T09:52:48+0000","dateFinished":"2019-06-19T09:52:48+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:19745"},{"text":"%md\n# Subcribe to a topic in Kafka\nWe indicate which topic in Kafka that we want to read data","user":"anonymous","dateUpdated":"2019-06-19T09:51:16+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>Subcribe to a topic in Kafka</h1>\n<p>We indicate which topic in Kafka that we want to read data</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1560937838805_-1872227194","id":"20190619-095038_1575211553","dateCreated":"2019-06-19T09:50:38+0000","dateStarted":"2019-06-19T09:51:16+0000","dateFinished":"2019-06-19T09:51:16+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:19746"},{"text":"// Subscribe to 1 topic\n// Here we use \"read\", not \"readStream\" because we used \"write\" when we write data to Kafka (because we write à Datafram, not a Stream), if we use \"writeStream\", here, we have to use \"readStream\"\nval df = spark\n  .read\n  .format(\"kafka\")\n  .option(\"kafka.bootstrap.servers\", \"hupi-factory-02-06-01.factory02.viet.cloud-torus.com:9092\")\n  .option(\"subscribe\", \"test_montus_factory02\")\n  .load()","user":"anonymous","dateUpdated":"2019-06-19T09:52:51+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"df: org.apache.spark.sql.DataFrame = [key: binary, value: binary ... 5 more fields]\n"}]},"apps":[],"jobName":"paragraph_1560934807790_60407044","id":"20190619-090007_1298877259","dateCreated":"2019-06-19T09:00:07+0000","dateStarted":"2019-06-19T09:52:51+0000","dateFinished":"2019-06-19T09:52:57+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:19747"},{"text":"df.printSchema()","user":"anonymous","dateUpdated":"2019-06-19T09:07:21+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"root\n |-- key: binary (nullable = true)\n |-- value: binary (nullable = true)\n |-- topic: string (nullable = true)\n |-- partition: integer (nullable = true)\n |-- offset: long (nullable = true)\n |-- timestamp: timestamp (nullable = true)\n |-- timestampType: integer (nullable = true)\n\n"}]},"apps":[],"jobName":"paragraph_1560935234942_673269125","id":"20190619-090714_1383614367","dateCreated":"2019-06-19T09:07:14+0000","dateStarted":"2019-06-19T09:07:21+0000","dateFinished":"2019-06-19T09:07:21+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:19748"},{"text":"z.show(df)","user":"anonymous","dateUpdated":"2019-06-19T09:53:00+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{"0":{"graph":{"mode":"table","height":300,"optionOpen":false,"setting":{"table":{"tableGridState":{},"tableColumnTypeState":{"names":{"key":"string","value":"string","topic":"string","partition":"string","offset":"string","timestamp":"string","timestampType":"string"},"updated":false},"tableOptionSpecHash":"[{\"name\":\"useFilter\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable filter for columns\"},{\"name\":\"showPagination\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable pagination for better navigation\"},{\"name\":\"showAggregationFooter\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable a footer for displaying aggregated values\"}]","tableOptionValue":{"useFilter":false,"showPagination":false,"showAggregationFooter":false},"updated":false,"initialized":false}},"commonSetting":{}}}},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TABLE","data":"key\tvalue\ttopic\tpartition\toffset\ttimestamp\ttimestampType\n[B@5619dd72\t[B@67fa7fbc\ttest_montus_factory02\t0\t0\t2019-06-19 00:27:04.481\t0\n[B@6030c97c\t[B@788c377c\ttest_montus_factory02\t0\t1\t2019-06-19 00:27:04.488\t0\n[B@60e69752\t[B@2be6bd50\ttest_montus_factory02\t0\t2\t2019-06-19 07:15:33.43\t0\n[B@7feee379\t[B@3dcc982\ttest_montus_factory02\t0\t3\t2019-06-19 07:15:33.439\t0\n[B@73390790\t[B@6d2b289a\ttest_montus_factory02\t0\t4\t2019-06-19 09:31:49.421\t0\n[B@65d6eb6d\t[B@a3be8c\ttest_montus_factory02\t0\t5\t2019-06-19 09:31:49.429\t0\n"},{"type":"TEXT","data":""}]},"runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://172.17.0.2:4040/jobs/job?id=0"],"interpreterSettingId":"spark"}},"apps":[],"jobName":"paragraph_1560935246696_-562200299","id":"20190619-090726_1518349356","dateCreated":"2019-06-19T09:07:26+0000","dateStarted":"2019-06-19T09:53:00+0000","dateFinished":"2019-06-19T09:53:02+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:19749"},{"text":"// If we just want to show 2 columns key and value\nval df_modified = df.selectExpr(\"CAST(key AS STRING)\", \"CAST(value AS STRING)\")\n\n// From here we have normal Dataframe that we can do all transformations that we want to ","user":"anonymous","dateUpdated":"2019-06-19T09:53:12+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"df_modified: org.apache.spark.sql.DataFrame = [key: string, value: string]\n"}]},"apps":[],"jobName":"paragraph_1560934911783_-509516086","id":"20190619-090151_1939903513","dateCreated":"2019-06-19T09:01:51+0000","dateStarted":"2019-06-19T09:53:12+0000","dateFinished":"2019-06-19T09:53:12+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:19750"},{"text":"z.show(df_modified)","user":"anonymous","dateUpdated":"2019-06-19T09:53:16+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{"0":{"graph":{"mode":"table","height":300,"optionOpen":false,"setting":{"table":{"tableGridState":{},"tableColumnTypeState":{"names":{"key":"string","value":"string"},"updated":false},"tableOptionSpecHash":"[{\"name\":\"useFilter\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable filter for columns\"},{\"name\":\"showPagination\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable pagination for better navigation\"},{\"name\":\"showAggregationFooter\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable a footer for displaying aggregated values\"}]","tableOptionValue":{"useFilter":false,"showPagination":false,"showAggregationFooter":false},"updated":false,"initialized":false}},"commonSetting":{}}}},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TABLE","data":"key\tvalue\n1\tThis is a test\n2\tFor Montus formation\n1\tThis is a test\n2\tFor Montus formation\n1\tThis is a test\n2\tFor Montus formation\n"},{"type":"TEXT","data":""}]},"runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://172.17.0.2:4040/jobs/job?id=1"],"interpreterSettingId":"spark"}},"apps":[],"jobName":"paragraph_1560934963531_1229676317","id":"20190619-090243_1959098833","dateCreated":"2019-06-19T09:02:43+0000","dateStarted":"2019-06-19T09:53:16+0000","dateFinished":"2019-06-19T09:53:16+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:19751"},{"user":"anonymous","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1560937996187_870927329","id":"20190619-095316_170232099","dateCreated":"2019-06-19T09:53:16+0000","status":"READY","progressUpdateIntervalMs":500,"$$hashKey":"object:19752"}],"name":"MONTUS/Formation - 3 Spark Streaming - read data from Kafka","id":"2EFCBD3RT","noteParams":{},"noteForms":{},"angularObjects":{"md:shared_process":[],"sh:shared_process":[],"spark:shared_process":[]},"config":{"isZeppelinNotebookCronEnable":false,"looknfeel":"default","personalizedMode":"false"},"info":{}}