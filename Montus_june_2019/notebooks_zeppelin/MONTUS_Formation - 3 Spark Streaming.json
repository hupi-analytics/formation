{"paragraphs":[{"text":"%md\nThis notebook aims to discover how to use Spark Streaming. \n\n\"Spark Streaming is an extension of the core Spark API that enables scalable, high-throughput, fault-tolerant stream processing of live data streams. Data can be ingested from many sources like Kafka, Flume, Kinesis, or TCP sockets, and can be processed using complex algorithms expressed with high-level functions like map, reduce, join and window. Finally, processed data can be pushed out to filesystems, databases, and live dashboards. In fact, you can apply Spark’s machine learning and graph processing algorithms on data streams.\" (source : spark.apache.org)\n\n***Reference link***\nhttps://spark.apache.org/docs/2.2.0/streaming-programming-guide.html\n","user":"anonymous","dateUpdated":"2019-06-14T14:43:32+0000","config":{"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"results":{},"enabled":true,"editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1560422217622_-1355940464","id":"20190610-090014_1338988644","dateCreated":"2019-06-13T10:36:57+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:5534","dateFinished":"2019-06-14T14:43:32+0000","dateStarted":"2019-06-14T14:43:32+0000","results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>This notebook aims to discover how to use Spark Streaming. </p>\n<p>&ldquo;Spark Streaming is an extension of the core Spark API that enables scalable, high-throughput, fault-tolerant stream processing of live data streams. Data can be ingested from many sources like Kafka, Flume, Kinesis, or TCP sockets, and can be processed using complex algorithms expressed with high-level functions like map, reduce, join and window. Finally, processed data can be pushed out to filesystems, databases, and live dashboards. In fact, you can apply Spark’s machine learning and graph processing algorithms on data streams.&rdquo; (source : spark.apache.org)</p>\n<p><strong><em>Reference link</em></strong><br/><a href=\"https://spark.apache.org/docs/2.2.0/streaming-programming-guide.html\">https://spark.apache.org/docs/2.2.0/streaming-programming-guide.html</a></p>\n</div>"}]}},{"text":"%md\n# Import libraries","user":"anonymous","dateUpdated":"2019-06-14T12:57:42+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1560516987534_1434925125","id":"20190614-125627_774768012","dateCreated":"2019-06-14T12:56:27+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:5725","dateFinished":"2019-06-14T12:57:42+0000","dateStarted":"2019-06-14T12:57:42+0000","results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>Import libraries</h1>\n</div>"}]}},{"text":"%spark.dep\nz.load(\"/opt/hupi/DATA/spark-streaming-kafka_2.10-1.5.2\")","user":"anonymous","dateUpdated":"2019-06-14T13:40:14+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1560517405511_142844311","id":"20190614-130325_2086890444","dateCreated":"2019-06-14T13:03:25+0000","status":"READY","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:6081"},{"text":"import org.apache.spark.streaming._\r\nimport org.apache.spark.streaming.StreamingContext._\r\nimport org.apache.spark.streaming.kafka._\r\nimport org.apache.spark.SparkConf","user":"anonymous","dateUpdated":"2019-06-14T13:01:00+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1560516983155_-481570861","id":"20190614-125623_547600067","dateCreated":"2019-06-14T12:56:23+0000","status":"ERROR","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:5650","dateFinished":"2019-06-14T13:01:00+0000","dateStarted":"2019-06-14T13:01:00+0000","results":{"code":"ERROR","msg":[{"type":"TEXT","data":"<console>:25: error: object kafka is not a member of package org.apache.spark.streaming\n       import org.apache.spark.streaming.kafka._\n                                         ^\n"}]}},{"user":"anonymous","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1560517133752_-1911087175","id":"20190614-125853_905711904","dateCreated":"2019-06-14T12:58:53+0000","status":"READY","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:5906","text":"//val sparkConf = new SparkConf().setAppName(\"DirectKafkaWordCount\")\r\nval sparkConf = new SparkConf().setAppName(\"Test Kafka2\") //sparkContext.getConf\r\nval ssc =  new StreamingContext(sparkContext, Seconds(10)) // window for every 10 seconds\r\nssc.checkpoint(\"checkpoint\")\r\n//val Array(zkQuorum, group, topics, numThreads) = args*/\r\nval zkQuorum = \"hupi-factory-02-02-05-01:2181\"\r\nval group = \"DEMO_HUPI_VINCENT\"\r\nval topics = \"factory02_water_sensors\"\r\nval numThreads = \"1\"","dateUpdated":"2019-06-14T13:40:38+0000"},{"user":"anonymous","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1560517099984_-2109528959","id":"20190614-125819_1071704045","dateCreated":"2019-06-14T12:58:19+0000","status":"READY","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:5818","text":"// Print What is read from Kafka code\r\nval topicMap = topics.split(\",\").map((_, numThreads.toInt)).toMap\r\nval streamdata = KafkaUtils.createStream(ssc, zkQuorum, group, topicMap)\r\nstreamdata.foreachRDD {\r\n  rdd => {\r\n    val lines = rdd.map(_._2)\r\n    println(\"lines pushed are \" + lines.count)\r\n  }\r\n}","dateUpdated":"2019-06-14T13:01:38+0000"},{"user":"anonymous","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1560517301935_-747302325","id":"20190614-130141_2102571234","dateCreated":"2019-06-14T13:01:41+0000","status":"READY","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:6003","text":"ssc.start()\r\nssc.awaitTermination()","dateUpdated":"2019-06-14T13:01:43+0000"}],"name":"MONTUS/Formation - 3 Spark Streaming","id":"2EFCBD3RT","noteParams":{},"noteForms":{},"angularObjects":{},"config":{"isZeppelinNotebookCronEnable":false,"looknfeel":"default","personalizedMode":"false"},"info":{}}