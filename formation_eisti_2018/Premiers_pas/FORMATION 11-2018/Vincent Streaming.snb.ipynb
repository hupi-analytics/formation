{
  "metadata" : {
    "id" : "b8efd722-104f-4952-b6e1-ccf4f0e88a0c",
    "name" : "Vincent Streaming",
    "user_save_timestamp" : "2018-11-20T15:25:19.165Z",
    "auto_save_timestamp" : "1970-01-01T00:00:00.000Z",
    "language_info" : {
      "name" : "scala",
      "file_extension" : "scala",
      "codemirror_mode" : "text/x-scala"
    },
    "trusted" : true,
    "sparkNotebook" : null,
    "customLocalRepo" : null,
    "customRepos" : null,
    "customDeps" : [
      "org.apache.spark % spark-streaming-kafka-0-8_2.11 % 2.2.0"
    ],
    "customImports" : null,
    "customArgs" : null,
    "customSparkConf" : null,
    "customVars" : null
  },
  "cells" : [
    {
      "metadata" : {
        "id" : "A47A0BCADB5B468E8A3C71B74CB86E2A"
      },
      "cell_type" : "markdown",
      "source" : "### Spark Streaming\n\nSpark Streaming is an extension of the core Spark API that enables scalable, high-throughput, \nfault-tolerant stream processing of live data streams. Data can be ingested from many sources like \nKafka, Flume, Kinesis, or TCP sockets, and can be processed using complex algorithms expressed \nwith high-level functions like map, reduce, join and window. Finally, processed data can be \npushed out to filesystems, databases, and live dashboards. In fact, you can apply Spark’s machine learning and graph processing algorithms on data streams."
    },
    {
      "metadata" : {
        "id" : "A47A0BCADB5B468E8A3C71B74CB86E2A"
      },
      "cell_type" : "markdown",
      "source" : "### Spark Streaming\n\nSpark Streaming is an extension of the core Spark API that enables scalable, high-throughput, \nfault-tolerant stream processing of live data streams. Data can be ingested from many sources like \nKafka, Flume, Kinesis, or TCP sockets, and can be processed using complex algorithms expressed \nwith high-level functions like map, reduce, join and window. Finally, processed data can be \npushed out to filesystems, databases, and live dashboards. In fact, you can apply Spark’s machine learning and graph processing algorithms on data streams."
    },
    {
      "metadata" : {
        "id" : "A47A0BCADB5B468E8A3C71B74CB86E2A"
      },
      "cell_type" : "markdown",
      "source" : "### Spark Streaming\n\nSpark Streaming is an extension of the core Spark API that enables scalable, high-throughput, \nfault-tolerant stream processing of live data streams. Data can be ingested from many sources like \nKafka, Flume, Kinesis, or TCP sockets, and can be processed using complex algorithms expressed \nwith high-level functions like map, reduce, join and window. Finally, processed data can be \npushed out to filesystems, databases, and live dashboards. In fact, you can apply Spark’s machine learning and graph processing algorithms on data streams."
    },
    {
      "metadata" : {
        "id" : "A47A0BCADB5B468E8A3C71B74CB86E2A"
      },
      "cell_type" : "markdown",
      "source" : "### Spark Streaming\n\nSpark Streaming is an extension of the core Spark API that enables scalable, high-throughput, \nfault-tolerant stream processing of live data streams. Data can be ingested from many sources like \nKafka, Flume, Kinesis, or TCP sockets, and can be processed using complex algorithms expressed \nwith high-level functions like map, reduce, join and window. Finally, processed data can be \npushed out to filesystems, databases, and live dashboards. In fact, you can apply Spark’s machine learning and graph processing algorithms on data streams."
    },
    {
      "metadata" : {
        "id" : "C5CFCAAC9A21424A83EB2A2A45EE3BF2"
      },
      "cell_type" : "markdown",
      "source" : "First, we import the names of the Spark Streaming classes and some implicit conversions from StreamingContext into our environment in order to add useful methods to other classes we need (like DStream). StreamingContext is the main entry point for all streaming functionality. We create a local StreamingContext with two execution threads, and a batch interval of 1 second."
    },
    {
      "metadata" : {
        "trusted" : true,
        "input_collapsed" : false,
        "collapsed" : false,
        "id" : "C9BB1D25CB1946838F4A2067B4187257"
      },
      "cell_type" : "code",
      "source" : [
        "import org.apache.spark.streaming._\n",
        "import org.apache.spark.streaming.StreamingContext._\n",
        "import org.apache.spark.streaming.kafka._\n",
        "import org.apache.spark.SparkConf"
      ],
      "outputs" : [
        {
          "name" : "stdout",
          "output_type" : "stream",
          "text" : "import org.apache.spark.streaming._\nimport org.apache.spark.streaming.StreamingContext._\nimport org.apache.spark.streaming.kafka._\nimport org.apache.spark.SparkConf\n"
        },
        {
          "metadata" : { },
          "data" : {
            "text/html" : ""
          },
          "output_type" : "execute_result",
          "execution_count" : 1,
          "time" : "Took: 1.374s, at 2018-11-21 10:48"
        }
      ]
    },
    {
      "metadata" : {
        "trusted" : true,
        "input_collapsed" : false,
        "collapsed" : false,
        "id" : "65BA9C6FF6DE45048DA430A0427F0C9D"
      },
      "cell_type" : "code",
      "source" : [
        "import scala.util.parsing.json._\n",
        "import play.api.libs.json._"
      ],
      "outputs" : [
        {
          "name" : "stdout",
          "output_type" : "stream",
          "text" : "import scala.util.parsing.json._\nimport play.api.libs.json._\n"
        },
        {
          "metadata" : { },
          "data" : {
            "text/html" : ""
          },
          "output_type" : "execute_result",
          "execution_count" : 2,
          "time" : "Took: 1.460s, at 2018-11-21 10:48"
        }
      ]
    },
    {
      "metadata" : {
        "trusted" : true,
        "input_collapsed" : false,
        "collapsed" : false,
        "id" : "A8C0CE835D564BF686EDBFB0F10CA91E"
      },
      "cell_type" : "code",
      "source" : [
        "//val sparkConf = new SparkConf().setAppName(\"DirectKafkaWordCount\")\n",
        "val sparkConf = new SparkConf().setAppName(\"Test Kafka2\") //sparkContext.getConf\n",
        "val ssc =  new StreamingContext(sparkContext, Seconds(10))\n",
        "ssc.checkpoint(\"checkpoint\")\n",
        "//val Array(zkQuorum, group, topics, numThreads) = args*/\n",
        "val zkQuorum = \"hupi-factory-02-02-05-01:2181\"\n",
        "val group = \"DEMO_HUPI_VINCENT\"\n",
        "val topics = \"factory02_hupilytics\"\n",
        "val numThreads = \"1\"\n",
        "val idsite = 1"
      ],
      "outputs" : [
        {
          "name" : "stdout",
          "output_type" : "stream",
          "text" : "sparkConf: org.apache.spark.SparkConf = org.apache.spark.SparkConf@7d0815cc\nssc: org.apache.spark.streaming.StreamingContext = org.apache.spark.streaming.StreamingContext@db4a172\nzkQuorum: String = hupi-factory-02-02-05-01:2181\ngroup: String = DEMO_HUPI_VINCENT\ntopics: String = factory02_hupilytics\nnumThreads: String = 1\nidsite: Int = 1\n"
        },
        {
          "metadata" : { },
          "data" : {
            "text/html" : ""
          },
          "output_type" : "execute_result",
          "execution_count" : 3,
          "time" : "Took: 1.352s, at 2018-11-21 10:48"
        }
      ]
    },
    {
      "metadata" : {
        "trusted" : true,
        "input_collapsed" : false,
        "collapsed" : false,
        "id" : "2AEAA458A4714D84B3C683BCC2420DF3"
      },
      "cell_type" : "code",
      "source" : [
        "// Print What is read from Kafka code\n",
        "val topicMap = topics.split(\",\").map((_, numThreads.toInt)).toMap\n",
        "val streamdata = KafkaUtils.createStream(ssc, zkQuorum, group, topicMap)\n",
        "streamdata.foreachRDD {\n",
        "  rdd => {\n",
        "    //val lines = rdd.map(_._2).map(myString => if (myString.contains(s\"\"\"\\\"idsite\\\":\"$idsite\"\"\")) {\n",
        "    //                                            myString\n",
        "    //                                          } else {\n",
        "    //                                            s\"This event has idsite different than $idsite\"\n",
        "     //                                         })\n",
        "    //println(\"lines pushed are \" + lines.count)\n",
        "    //lines.collect().foreach(println)\n",
        "    //val df = rdd.toDF()\n",
        "    val maliste: List[String] = List(\"10\")\n",
        "\n",
        "    val pidVidList = rdd.map(l => JSON.parseFull(l._2))\n",
        "        .map(l => l match {\n",
        "            case Some(a: Map[String, Any]) => a.toMap\n",
        "            // case a => a.toMap//Map[String,String]()\n",
        "            case _ => Map[String, String]()\n",
        "            })\n",
        "        .filter(l => ( maliste.contains(l.getOrElse(\"idsite\", \"0\"))))\n",
        "        .map(l =>\n",
        "          if (l.getOrElse(\"idgoal\", \"-1\")!=\"0\")\n",
        "          (l.getOrElse(\"_pks\", 0).toString, l.getOrElse(\"_id\", \"none\").toString, l.getOrElse(\"uid\", 0).toString, l.getOrElse(\"current_ts\", \"none\").toString, l.getOrElse(\"idsite\", 0))\n",
        "          else if (!l.contains(\"ec_id\"))\n",
        "          ((Json.parse(l.getOrElse(\"ec_items\", \"none\").toString), l.getOrElse(\"_id\", \"none\").toString, l.getOrElse(\"uid\", 0).toString, l.getOrElse(\"current_ts\", \"none\").toString, l.getOrElse(\"idgoal\", \"-1\"), l.getOrElse(\"idsite\", 0)))\n",
        "          else\n",
        "          ((Json.parse(l.getOrElse(\"ec_items\", \"none\").toString), l.getOrElse(\"_id\", \"none\").toString, l.getOrElse(\"uid\", 0).toString, l.getOrElse(\"current_ts\", \"none\").toString, l.getOrElse(\"idgoal\", \"-1\"), \"Purchase\", l.getOrElse(\"idsite\", 0)))\n",
        "          )\n",
        "    \n",
        "      pidVidList.collect().foreach(println)\n",
        "    \n",
        "    \n",
        "  }\n",
        "}"
      ],
      "outputs" : [
        {
          "name" : "stdout",
          "output_type" : "stream",
          "text" : "<console>:108: warning: non-variable type argument String in type pattern scala.collection.immutable.Map[String,Any] (the underlying of Map[String,Any]) is unchecked since it is eliminated by erasure\n            case Some(a: Map[String, Any]) => a.toMap\n                         ^\ntopicMap: scala.collection.immutable.Map[String,Int] = Map(factory02_hupilytics -> 1)\nstreamdata: org.apache.spark.streaming.dstream.ReceiverInputDStream[(String, String)] = org.apache.spark.streaming.kafka.KafkaInputDStream@6f97cf14\n"
        },
        {
          "metadata" : { },
          "data" : {
            "text/html" : ""
          },
          "output_type" : "execute_result",
          "execution_count" : 4,
          "time" : "Took: 3.163s, at 2018-11-21 10:48"
        }
      ]
    },
    {
      "metadata" : {
        "trusted" : true,
        "input_collapsed" : false,
        "collapsed" : true,
        "id" : "C2384F65AA764F988E4E47C19BECB27B"
      },
      "cell_type" : "code",
      "source" : [
        "ssc.start()\n",
        "ssc.awaitTermination()"
      ],
      "outputs" : [ ]
    },
    {
      "metadata" : {
        "trusted" : true,
        "input_collapsed" : false,
        "collapsed" : true,
        "id" : "1982B9D867AF4342BFA2B3CA7FE0280C"
      },
      "cell_type" : "code",
      "source" : [
        "ssc.stop()"
      ],
      "outputs" : [ ]
    },
    {
      "metadata" : {
        "trusted" : true,
        "input_collapsed" : false,
        "collapsed" : false,
        "id" : "56A5722FD2DB4CC283065B813E9E4029"
      },
      "cell_type" : "code",
      "source" : [
        ""
      ],
      "outputs" : [
        {
          "metadata" : { },
          "data" : {
            "text/html" : ""
          },
          "output_type" : "execute_result",
          "execution_count" : 6,
          "time" : "Took: 1.182s, at 2018-11-20 15:56"
        }
      ]
    }
  ],
  "nbformat" : 4
}