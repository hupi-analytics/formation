{"metadata":{"id":"dcd109f0-2022-44e4-a46f-971218621f4f","name":"Formation 4 - ML1 - Regression_cyu1","user_save_timestamp":"1970-01-01T00:00:00.000Z","auto_save_timestamp":"1970-01-01T00:00:00.000Z","language_info":{"name":"scala","file_extension":"scala","codemirror_mode":"text/x-scala"},"trusted":true,"sparkNotebook":null,"customLocalRepo":null,"customRepos":null,"customDeps":null,"customImports":null,"customArgs":null,"customSparkConf":null,"customVars":null},"cells":[{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":false,"id":"130B34EFC82E4DD8883AD01DA7695EEC"},"cell_type":"markdown","source":"<img src=http://fd.perso.eisti.fr/Logos/TORUS2.png>"},{"metadata":{"id":"5F687656ADF14F5B95AA9F6C8DF0035D"},"cell_type":"markdown","source":"In this section, let us present to you some Machine Learning algorithms, there are many, but 3 algorithms below can be considered as the most popular in Machine Learning :\n\n- 1/ Regression - Linear Regression\n- 2/ Classification - Random Forest\n- 3/ Clustering - KMeans\n\nThis notebook will focus on the first one, we'll take a dataset and then build a linear regression model based on it. \n\n\"Linear regression is the most basic type of regression and commonly used predictive analysis.  The overall idea of regression is to examine two things: (1) does a set of predictor variables do a good job in predicting an outcome variable?  Is the model using the predictors accounting for the variability in the changes in the dependent variable? (2) Which variables in particular are significant predictors of the dependent variable?  And in what way do they--indicated by the magnitude and sign of the beta estimates--impact the dependent variable?  These regression estimates are used to explain the relationship between one dependent variable and one or more independent variables. (3) What is the regression equation that shows how the set of predictor variables can be used to predict the outcome?  The simplest form of the equation with one dependent and one independent variable is defined by the formula y = c + b*x, where y = estimated dependent score, c = constant, b = regression coefficients, and x = independent variable.\"\n\n(source : http://www.statisticssolutions.com/what-is-linear-regression/)"},{"metadata":{"id":"69E1DBB6815249F695137621B1D60C2B"},"cell_type":"markdown","source":"### Read dataset (csv format) from HDFS\n\nHere we use the dataset from http://www.statsci.org/data/general/water.html \n\nThe target variable will be monthly water usage (gallons) and the variables descriptives are : \n- Average monthly temperature (F)\n- Amount of production (M pounds)\n- Number of plant operating days in the month\n- Number of persons on the monthly plant payroll"},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":false,"presentation":{"tabs_state":"{\n  \"tab_id\": \"#tab1599598013-0\"\n}","pivot_chart_state":"{\n  \"hiddenAttributes\": [],\n  \"menuLimit\": 200,\n  \"cols\": [],\n  \"rows\": [],\n  \"vals\": [],\n  \"exclusions\": {},\n  \"inclusions\": {},\n  \"unusedAttrsVertical\": 85,\n  \"autoSortUnusedAttrs\": false,\n  \"inclusionsInfo\": {},\n  \"aggregatorName\": \"Count\",\n  \"rendererName\": \"Table\"\n}"},"id":"7D20B9182CD34E1887E1AC69C21C695D"},"cell_type":"code","source":["import org.apache.spark.sql._            \n","val spark = SparkSession.builder().getOrCreate()\n","\n","val data = spark.read.format(\"com.databricks.spark.csv\")\n","                .option(\"header\", \"true\")\n","                .option(\"inferSchema\", \"true\") \n","                .load(\"hdfs://hupi-factory-02-01-01-01/user/hupi/dataset_torusVN/formation4_ML/water.csv\")"],"outputs":[{"metadata":{},"data":{"text/html":""},"output_type":"execute_result","execution_count":3,"time":"Took: 1.125s, at 2022-02-24 10:04"}]},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":false,"id":"F6E7695F7D174FA980D38A9012AB0D83"},"cell_type":"code","source":["data.show()"],"outputs":[{"name":"stdout","output_type":"stream","text":"+-----------+----------+----+-------+-----+\n|Temperature|Production|Days|Persons|Water|\n+-----------+----------+----+-------+-----+\n|       58.8|      7107|  21|    129| 3067|\n|       65.2|      6373|  22|    141| 2828|\n|       70.9|      6796|  22|    153| 2891|\n|       77.4|      9208|  20|    166| 2994|\n|       79.3|     14792|  25|    193| 3082|\n|       81.0|     14564|  23|    189| 3898|\n|       71.9|     11964|  20|    175| 3502|\n|       63.9|     13526|  23|    186| 3060|\n|       54.5|     12656|  20|    190| 3211|\n|       39.5|     14119|  20|    187| 3286|\n|       44.5|     16691|  22|    195| 3542|\n|       43.6|     14571|  19|    206| 3125|\n|       56.0|     13619|  22|    198| 3022|\n|       64.7|     14575|  22|    192| 2922|\n|       73.0|     14556|  21|    191| 3950|\n|       78.9|     18573|  21|    200| 4488|\n|       79.4|     15618|  22|    200| 3295|\n+-----------+----------+----+-------+-----+\n\n"},{"metadata":{},"data":{"text/html":""},"output_type":"execute_result","execution_count":4,"time":"Took: 1.137s, at 2022-02-24 10:04"}]},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":false,"id":"3464411AEED342B1BA023CCE9FE89CBC"},"cell_type":"code","source":["data.printSchema()"],"outputs":[{"name":"stdout","output_type":"stream","text":"root\n |-- Temperature: double (nullable = true)\n |-- Production: integer (nullable = true)\n |-- Days: integer (nullable = true)\n |-- Persons: integer (nullable = true)\n |-- Water: integer (nullable = true)\n\n"},{"metadata":{},"data":{"text/html":""},"output_type":"execute_result","execution_count":5,"time":"Took: 1.155s, at 2022-02-24 10:07"}]},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":false,"id":"EA65B7CA43B245F594BAD870292C709D"},"cell_type":"code","source":["data.describe()"],"outputs":[{"name":"stdout","output_type":"stream","text":"res23: org.apache.spark.sql.DataFrame = [summary: string, Temperature: string ... 4 more fields]\n"},{"metadata":{},"data":{"text/html":"<div class=\"df-canvas\">\n      <script data-this=\"{&quot;dataId&quot;:&quot;anoncbefaf24df39be9fdca1a87584f93c4b&quot;,&quot;partitionIndexId&quot;:&quot;anonb1889a38c93a4c705d82e7dd310831b1&quot;,&quot;numPartitions&quot;:1,&quot;dfSchema&quot;:{&quot;type&quot;:&quot;struct&quot;,&quot;fields&quot;:[{&quot;name&quot;:&quot;summary&quot;,&quot;type&quot;:&quot;string&quot;,&quot;nullable&quot;:true,&quot;metadata&quot;:{}},{&quot;name&quot;:&quot;Temperature&quot;,&quot;type&quot;:&quot;string&quot;,&quot;nullable&quot;:true,&quot;metadata&quot;:{}},{&quot;name&quot;:&quot;Production&quot;,&quot;type&quot;:&quot;string&quot;,&quot;nullable&quot;:true,&quot;metadata&quot;:{}},{&quot;name&quot;:&quot;Days&quot;,&quot;type&quot;:&quot;string&quot;,&quot;nullable&quot;:true,&quot;metadata&quot;:{}},{&quot;name&quot;:&quot;Persons&quot;,&quot;type&quot;:&quot;string&quot;,&quot;nullable&quot;:true,&quot;metadata&quot;:{}},{&quot;name&quot;:&quot;Water&quot;,&quot;type&quot;:&quot;string&quot;,&quot;nullable&quot;:true,&quot;metadata&quot;:{}}]}}\" type=\"text/x-scoped-javascript\">/*<![CDATA[*/req(['../javascripts/notebook/dataframe','../javascripts/notebook/consoleDir'], \n      function(dataframe, extension) {\n        dataframe.call(data, this, extension);\n      }\n    );/*]]>*/</script>\n      <link rel=\"stylesheet\" href=\"/assets/stylesheets/ipython/css/dataframe.css\" type=\"text/css\"/>\n    </div>"},"output_type":"execute_result","execution_count":6,"time":"Took: 1.313s, at 2022-02-24 10:08"}]},{"metadata":{"id":"968621DB51114A018B9994922D915DCB"},"cell_type":"markdown","source":"### Some descriptions of data"},{"metadata":{"id":"300A0B9D59314F1DAA271283F409F1AE"},"cell_type":"markdown","source":"#### Statistics summary "},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":false,"id":"C4857D0329C5474687D432EDAA83889F"},"cell_type":"code","source":["import org.apache.spark.mllib.linalg.Vectors\n","import org.apache.spark.mllib.stat.{MultivariateStatisticalSummary, Statistics}"],"outputs":[{"name":"stdout","output_type":"stream","text":"import org.apache.spark.mllib.linalg.Vectors\nimport org.apache.spark.mllib.stat.{MultivariateStatisticalSummary, Statistics}\n"},{"metadata":{},"data":{"text/html":""},"output_type":"execute_result","execution_count":14,"time":"Took: 0.675s, at 2022-02-24 10:27"}]},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":false,"id":"E3956920DEFB424B8160B3E83D37B9E2"},"cell_type":"code","source":["// Convert df to RDD to be able to use the library MultiVariateStatisticalSummary.\n","val rdd = data.map(l => (l(0).asInstanceOf[Double], l(1).asInstanceOf[Integer].toDouble, l(2).asInstanceOf[Integer].toDouble,\n","                        l(3).asInstanceOf[Integer].toDouble, l(4).asInstanceOf[Integer].toDouble)).rdd"],"outputs":[{"metadata":{},"data":{"text/html":""},"output_type":"execute_result","execution_count":8,"time":"Took: 1.549s, at 2022-02-24 10:05"}]},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":false,"presentation":{"tabs_state":"{\n  \"tab_id\": \"#tab1320546478-0\"\n}","pivot_chart_state":"{\n  \"hiddenAttributes\": [],\n  \"menuLimit\": 200,\n  \"cols\": [],\n  \"rows\": [],\n  \"vals\": [],\n  \"exclusions\": {},\n  \"inclusions\": {},\n  \"unusedAttrsVertical\": 85,\n  \"autoSortUnusedAttrs\": false,\n  \"inclusionsInfo\": {},\n  \"aggregatorName\": \"Count\",\n  \"rendererName\": \"Table\"\n}"},"id":"00528782904447488936CBFDC883ECE7"},"cell_type":"code","source":["rdd.take(2)"],"outputs":[{"name":"stdout","output_type":"stream","text":"res40: Array[(Double, Double, Double, Double, Double)] = Array((58.8,7107.0,21.0,129.0,3067.0), (65.2,6373.0,22.0,141.0,2828.0))\n"},{"metadata":{},"data":{"text/html":"<div>\n      <script data-this=\"{&quot;dataId&quot;:&quot;anoned64a67590199baf4f890f774fd8dd40&quot;,&quot;dataInit&quot;:[],&quot;genId&quot;:&quot;1320546478&quot;}\" type=\"text/x-scoped-javascript\">/*<![CDATA[*/req(['../javascripts/notebook/playground','../javascripts/notebook/magic/tabs'], \n      function(playground, _magictabs) {\n        // data ==> data-this (in observable.js's scopedEval) ==> this in JS => { dataId, dataInit, ... }\n        // this ==> scope (in observable.js's scopedEval) ==> this.parentElement ==> div.container below (toHtml)\n\n        playground.call(data,\n                        this\n                        ,\n                        {\n    \"f\": _magictabs,\n    \"o\": {}\n  }\n  \n                        \n                        \n                      );\n      }\n    );/*]]>*/</script>\n    <div>\n      <div>\n        <ul class=\"nav nav-tabs\" id=\"ul1320546478\"><li>\n              <a href=\"#tab1320546478-0\"><i class=\"fa fa-table\"/></a>\n            </li><li>\n              <a href=\"#tab1320546478-1\"><i class=\"fa fa-cubes\"/></a>\n            </li></ul>\n\n        <div class=\"tab-content\" id=\"tab1320546478\"><div class=\"tab-pane\" id=\"tab1320546478-0\">\n            <div>\n      <script data-this=\"{&quot;dataId&quot;:&quot;anon0062f3d8884a091dd382ef00f68e96c6&quot;,&quot;dataInit&quot;:[{&quot;_3&quot;:21,&quot;_2&quot;:7107,&quot;_1&quot;:58.8,&quot;_4&quot;:129,&quot;_5&quot;:3067},{&quot;_3&quot;:22,&quot;_2&quot;:6373,&quot;_1&quot;:65.2,&quot;_4&quot;:141,&quot;_5&quot;:2828}],&quot;genId&quot;:&quot;1720442&quot;}\" type=\"text/x-scoped-javascript\">/*<![CDATA[*/req(['../javascripts/notebook/playground','../javascripts/notebook/magic/tableChart'], \n      function(playground, _magictableChart) {\n        // data ==> data-this (in observable.js's scopedEval) ==> this in JS => { dataId, dataInit, ... }\n        // this ==> scope (in observable.js's scopedEval) ==> this.parentElement ==> div.container below (toHtml)\n\n        playground.call(data,\n                        this\n                        ,\n                        {\n    \"f\": _magictableChart,\n    \"o\": {\"headers\":[\"_1\",\"_2\",\"_3\",\"_4\",\"_5\"],\"width\":600,\"height\":400}\n  }\n  \n                        \n                        \n                      );\n      }\n    );/*]]>*/</script>\n    <div>\n      <span class=\"chart-total-item-count\"><p data-bind=\"text: value\"><script data-this=\"{&quot;valueId&quot;:&quot;anon13807aef6ae9212a94a0d0f4934cec3a&quot;,&quot;initialValue&quot;:&quot;2&quot;}\" type=\"text/x-scoped-javascript\">/*<![CDATA[*/\nreq(\n['observable', 'knockout'],\nfunction (O, ko) {\n  ko.applyBindings({\n      value: O.makeObservable(valueId, initialValue)\n    },\n    this\n  );\n});\n        /*]]>*/</script></p> entries total</span>\n      <span class=\"chart-sampling-warning\"><p data-bind=\"text: value\"><script data-this=\"{&quot;valueId&quot;:&quot;anon371eaeca2b0838e9788af3ab7188d1d7&quot;,&quot;initialValue&quot;:&quot;&quot;}\" type=\"text/x-scoped-javascript\">/*<![CDATA[*/\nreq(\n['observable', 'knockout'],\nfunction (O, ko) {\n  ko.applyBindings({\n      value: O.makeObservable(valueId, initialValue)\n    },\n    this\n  );\n});\n        /*]]>*/</script></p></span>\n      <div>\n      </div>\n    </div></div>\n            </div><div class=\"tab-pane\" id=\"tab1320546478-1\">\n            <div>\n      <script data-this=\"{&quot;dataId&quot;:&quot;anon8f1467e0b0f7adce57737578ff34c6d5&quot;,&quot;dataInit&quot;:[{&quot;_3&quot;:21,&quot;_2&quot;:7107,&quot;_1&quot;:58.8,&quot;_4&quot;:129,&quot;_5&quot;:3067},{&quot;_3&quot;:22,&quot;_2&quot;:6373,&quot;_1&quot;:65.2,&quot;_4&quot;:141,&quot;_5&quot;:2828}],&quot;genId&quot;:&quot;1206598489&quot;}\" type=\"text/x-scoped-javascript\">/*<![CDATA[*/req(['../javascripts/notebook/playground','../javascripts/notebook/magic/pivotChart'], \n      function(playground, _magicpivotChart) {\n        // data ==> data-this (in observable.js's scopedEval) ==> this in JS => { dataId, dataInit, ... }\n        // this ==> scope (in observable.js's scopedEval) ==> this.parentElement ==> div.container below (toHtml)\n\n        playground.call(data,\n                        this\n                        ,\n                        {\n    \"f\": _magicpivotChart,\n    \"o\": {\"width\":600,\"height\":400,\"derivedAttributes\":{},\"extraOptions\":{}}\n  }\n  \n                        \n                        \n                      );\n      }\n    );/*]]>*/</script>\n    <div>\n      <span class=\"chart-total-item-count\"><p data-bind=\"text: value\"><script data-this=\"{&quot;valueId&quot;:&quot;anone9756c39a3dea900ef8132a830797c71&quot;,&quot;initialValue&quot;:&quot;2&quot;}\" type=\"text/x-scoped-javascript\">/*<![CDATA[*/\nreq(\n['observable', 'knockout'],\nfunction (O, ko) {\n  ko.applyBindings({\n      value: O.makeObservable(valueId, initialValue)\n    },\n    this\n  );\n});\n        /*]]>*/</script></p> entries total</span>\n      <span class=\"chart-sampling-warning\"><p data-bind=\"text: value\"><script data-this=\"{&quot;valueId&quot;:&quot;anonb513d58cd0d038dc81390533bcd3279f&quot;,&quot;initialValue&quot;:&quot;&quot;}\" type=\"text/x-scoped-javascript\">/*<![CDATA[*/\nreq(\n['observable', 'knockout'],\nfunction (O, ko) {\n  ko.applyBindings({\n      value: O.makeObservable(valueId, initialValue)\n    },\n    this\n  );\n});\n        /*]]>*/</script></p></span>\n      <div>\n      </div>\n    </div></div>\n            </div></div>\n      </div>\n    </div></div>"},"output_type":"execute_result","execution_count":16,"time":"Took: 0.937s, at 2022-02-24 10:28"}]},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":false,"id":"CF81287EF5554647B4B1565D361E2A52"},"cell_type":"code","source":["// Convert rdd to the rdd of vectors\n","val observations = rdd.map(l => Vectors.dense(l._1, l._2, l._3, l._4, l._5))"],"outputs":[{"name":"stdout","output_type":"stream","text":"observations: org.apache.spark.rdd.RDD[org.apache.spark.mllib.linalg.Vector] = MapPartitionsRDD[63] at map at <console>:86\n"},{"metadata":{},"data":{"text/html":""},"output_type":"execute_result","execution_count":18,"time":"Took: 0.879s, at 2022-02-24 10:28"}]},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":false,"id":"7F008D99219D49D983E6E2D8E396B785"},"cell_type":"code","source":["// Compute column summary statistics.\n","val summary: MultivariateStatisticalSummary = Statistics.colStats(observations)\n","println(\"Vectors of observations' mean : \" + summary.mean)  \n","println(\"Vectors of observations' variance : \" + summary.variance)  \n","println(\"Vectors of observations' number of column not null : \" + summary.numNonzeros)  \n","println()"],"outputs":[{"name":"stdout","output_type":"stream","text":"Vectors of observations' mean : [64.8529411764706,12900.470588235294,21.470588235294116,181.8235294117647,3303.705882352941]\nVectors of observations' variance : [182.52264705882362,1.2438223764705881E7,2.1397058823529416,483.7794117647057,199539.47058823524]\nVectors of observations' number of column not null : [17.0,17.0,17.0,17.0,17.0]\n\nsummary: org.apache.spark.mllib.stat.MultivariateStatisticalSummary = org.apache.spark.mllib.stat.MultivariateOnlineSummarizer@6c85eb06\n"},{"metadata":{},"data":{"text/html":""},"output_type":"execute_result","execution_count":19,"time":"Took: 1.158s, at 2022-02-24 10:28"}]},{"metadata":{"id":"2D5A83819A804487BEDDAEC5046EB347"},"cell_type":"markdown","source":"#### Correlations of variables "},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":false,"id":"D2CC7BB5AAED4C2F83373CCE36EB5FBB"},"cell_type":"code","source":["import org.apache.spark.mllib.linalg._\n","import org.apache.spark.mllib.stat.Statistics\n","import org.apache.spark.rdd.RDD"],"outputs":[{"name":"stdout","output_type":"stream","text":"import org.apache.spark.mllib.linalg._\nimport org.apache.spark.mllib.stat.Statistics\nimport org.apache.spark.rdd.RDD\n"},{"metadata":{},"data":{"text/html":""},"output_type":"execute_result","execution_count":20,"time":"Took: 0.659s, at 2022-02-24 10:28"}]},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":false,"id":"48C31D5093B44B0A9CE0F7B431C48CB0"},"cell_type":"code","source":["// calculate the correlation matrix using Pearson's method. Use \"spearman\" for Spearman's method\n","// If a method is not specified, Pearson's method will be used by default.\n","val correlMatrix: Matrix = Statistics.corr(observations, \"pearson\")\n","println(correlMatrix.toString)"],"outputs":[{"name":"stdout","output_type":"stream","text":"1.0                   -0.02410741870356305  0.43762975958335126   ... (5 total)\n-0.02410741870356305  1.0                   0.10573054707596519   ...\n0.43762975958335126   0.10573054707596519   1.0                   ...\n-0.08205777488270032  0.9184797375869633    0.03188119325449726   ...\n0.28575755805713965   0.6307494802500775    -0.08882582642644302  ...\ncorrelMatrix: org.apache.spark.mllib.linalg.Matrix =\n1.0                   -0.02410741870356305  0.43762975958335126   ... (5 total)\n-0.02410741870356305  1.0                   0.10573054707596519   ...\n0.43762975958335126   0.10573054707596519   1.0                   ...\n-0.08205777488270032  0.9184797375869633    0.03188119325449726   ...\n0.28575755805713965   0.6307494802500775    -0.08882582642644302  ...\n"},{"metadata":{},"data":{"text/html":""},"output_type":"execute_result","execution_count":33,"time":"Took: 1.042s, at 2022-02-24 10:32"}]},{"metadata":{"id":"7CB039FA56624D5A8B1CC54103094951"},"cell_type":"markdown","source":"In this example, we don't have many variables descriptives, so we suppose that we can use all variables to build the regression model. Otherwise, we need to do a selection of variables to select the variables that affect the most the target variable. To do selection variable, depending on the type of variables, we can use different methods. In Spark, we have some basic tools to do that, for example https://spark.apache.org/docs/latest/ml-features.html#feature-selectors "},{"metadata":{"id":"9E0FE30DA33C419B8FD513684AA77990"},"cell_type":"markdown","source":"###  Vector Assembler\n\nTo prepare for the construction of linear regression by using ML library, we have to have a data with 2 columns only (\"label\" and \"features\"). To have that, we need to put all the variables descriptives into a single vector column named \"features\" and column of the target variable should be renamed to \"label\". "},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":false,"id":"C2A90C5B8CBE41D88975BFA9302B9544"},"cell_type":"code","source":["import org.apache.spark.ml.feature.VectorAssembler\n","import org.apache.spark.ml.linalg.Vectors"],"outputs":[{"metadata":{},"data":{"text/html":""},"output_type":"execute_result","execution_count":52,"time":"Took: 0.624s, at 2022-02-24 10:43"},{"name":"stdout","output_type":"stream","text":"import org.apache.spark.ml.feature.VectorAssembler\nimport org.apache.spark.ml.linalg.Vectors\n"},{"metadata":{},"data":{"text/html":""},"output_type":"execute_result","execution_count":53,"time":"Took: 0.629s, at 2022-02-24 10:43"}]},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":false,"id":"36CC3C48CB6842358183FEA88AEF9760"},"cell_type":"code","source":["val assembler = new VectorAssembler()\n","  .setInputCols(Array(\"Temperature\", \"Production\", \"Days\", \"Persons\"))\n","  .setOutputCol(\"features\")"],"outputs":[{"name":"stdout","output_type":"stream","text":"assembler: org.apache.spark.ml.feature.VectorAssembler = vecAssembler_dc3154249315\n"},{"metadata":{},"data":{"text/html":""},"output_type":"execute_result","execution_count":58,"time":"Took: 0.717s, at 2022-02-24 10:43"}]},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":false,"id":"1121BE3C105944A09C40AC40C4D5A226"},"cell_type":"code","source":["val training = assembler.transform(data)\n","                        .select(\"Water\", \"features\")\n","                        .withColumnRenamed(\"Water\", \"label\")"],"outputs":[{"name":"stdout","output_type":"stream","text":"training: org.apache.spark.sql.DataFrame = [label: int, features: vector]\n"},{"metadata":{},"data":{"text/html":""},"output_type":"execute_result","execution_count":59,"time":"Took: 0.996s, at 2022-02-24 10:43"}]},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":false,"id":"5204FA54F7C14F6DABEF205D8ECD1E43"},"cell_type":"code","source":["val Array(train, test) = data.randomSplit(Array(0.8, 0.2))"],"outputs":[{"name":"stdout","output_type":"stream","text":"train: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [Temperature: double, Production: int ... 3 more fields]\ntest: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [Temperature: double, Production: int ... 3 more fields]\n"},{"metadata":{},"data":{"text/html":""},"output_type":"execute_result","execution_count":60,"time":"Took: 0.927s, at 2022-02-24 10:43"}]},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":false,"id":"D5F10ECEC11D4AF8AD5D70D37338361F"},"cell_type":"code","source":["test.show()"],"outputs":[{"name":"stdout","output_type":"stream","text":"+-----------+----------+----+-------+-----+\n|Temperature|Production|Days|Persons|Water|\n+-----------+----------+----+-------+-----+\n|       54.5|     12656|  20|    190| 3211|\n|       56.0|     13619|  22|    198| 3022|\n|       58.8|      7107|  21|    129| 3067|\n|       79.3|     14792|  25|    193| 3082|\n|       79.4|     15618|  22|    200| 3295|\n+-----------+----------+----+-------+-----+\n\n"},{"metadata":{},"data":{"text/html":""},"output_type":"execute_result","execution_count":61,"time":"Took: 1.272s, at 2022-02-24 10:43"}]},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":false,"id":"91FC649096724D02962489CC32FAE522"},"cell_type":"code","source":["training.show()"],"outputs":[{"name":"stdout","output_type":"stream","text":"+-----+--------------------+\n|label|            features|\n+-----+--------------------+\n| 3067|[58.8,7107.0,21.0...|\n| 2828|[65.2,6373.0,22.0...|\n| 2891|[70.9,6796.0,22.0...|\n| 2994|[77.4,9208.0,20.0...|\n| 3082|[79.3,14792.0,25....|\n| 3898|[81.0,14564.0,23....|\n| 3502|[71.9,11964.0,20....|\n| 3060|[63.9,13526.0,23....|\n| 3211|[54.5,12656.0,20....|\n| 3286|[39.5,14119.0,20....|\n| 3542|[44.5,16691.0,22....|\n| 3125|[43.6,14571.0,19....|\n| 3022|[56.0,13619.0,22....|\n| 2922|[64.7,14575.0,22....|\n| 3950|[73.0,14556.0,21....|\n| 4488|[78.9,18573.0,21....|\n| 3295|[79.4,15618.0,22....|\n+-----+--------------------+\n\n"},{"metadata":{},"data":{"text/html":""},"output_type":"execute_result","execution_count":62,"time":"Took: 1.187s, at 2022-02-24 10:43"}]},{"metadata":{"id":"394ED2D646624DA79B3936C16F94A809"},"cell_type":"markdown","source":"### Build a linear regression model \n\nTo have the best model, we can try to fluctuate the parameters such as : number of max iterations, regularization parameters, etc. To find all the parameters supported by Spark that we can play with, you can see it in : https://spark.apache.org/docs/1.6.2/api/scala/index.html#org.apache.spark.ml.regression.LinearRegression"},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":false,"id":"E305D86AF212465C88BDFF8B23175D95"},"cell_type":"code","source":["import org.apache.spark.ml.regression.LinearRegression\n","\n","val lr = new LinearRegression()\n","  .setMaxIter(10)\n","  .setRegParam(0.3)\n","  .setElasticNetParam(0.8)\n","\n","// Fit the model\n","val lrModel = lr.fit(training)"],"outputs":[{"metadata":{},"data":{"text/html":""},"output_type":"execute_result","execution_count":63,"time":"Took: 1.669s, at 2022-02-24 10:44"},{"name":"stdout","output_type":"stream","text":"import org.apache.spark.ml.regression.LinearRegression\nlr: org.apache.spark.ml.regression.LinearRegression = linReg_f4f69c802e71\nlrModel: org.apache.spark.ml.regression.LinearRegressionModel = linReg_f4f69c802e71\n"},{"metadata":{},"data":{"text/html":""},"output_type":"execute_result","execution_count":64,"time":"Took: 1.041s, at 2022-02-24 10:44"}]},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":false,"id":"B2C1EF47B7DD4BBBA36FDE5A4ED36BE6"},"cell_type":"code","source":["// Print the coefficients and intercept for linear regression\n","println(s\"Coefficients: ${lrModel.coefficients} Intercept: ${lrModel.intercept}\")"],"outputs":[{"name":"stdout","output_type":"stream","text":"Coefficients: [14.327053237030215,0.1953629027869717,-128.50694198665602,-18.495546085518836] Intercept: 5976.326064543363\n"},{"metadata":{},"data":{"text/html":""},"output_type":"execute_result","execution_count":65,"time":"Took: 1.007s, at 2022-02-24 10:44"}]},{"metadata":{"id":"5DAB8E2EDCFF4A80A8A4DDC0C03074A4"},"cell_type":"markdown","source":"### Evaluation of model \n\nSome other metrics that can be computed : https://spark.apache.org/docs/1.6.2/api/scala/index.html#org.apache.spark.ml.regression.LinearRegressionTrainingSummary"},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":false,"id":"64614EC412164C5191C232908B63E699"},"cell_type":"code","source":["// Summarize the model over the training set and print out some metrics\n","val trainingSummary = lrModel.summary\n","println(s\"numIterations: ${trainingSummary.totalIterations}\")\n","println(s\"objectiveHistory: [${trainingSummary.objectiveHistory.mkString(\",\")}]\")\n","trainingSummary.residuals.show()\n","println(s\"RMSE: ${trainingSummary.rootMeanSquaredError}\")\n","println(s\"r2: ${trainingSummary.r2}\")"],"outputs":[{"name":"stdout","output_type":"stream","text":"numIterations: 11\nobjectiveHistory: [0.5000000000000036,0.4615658864385346,0.25876938382285175,0.23163114088245704,0.22711753479176247,0.1808987988418104,0.16024337290053345,0.13534375971375456,0.1271681430235667,0.12544451188702493,0.1208688800121053]\n+-------------------+\n|          residuals|\n+-------------------+\n|-55.629718236040844|\n| 107.52700670548484|\n| 228.17084840174948|\n|   -249.74210402269|\n|-137.95550009320868|\n| 367.23518292388235|\n| -34.90355653024835|\n| -78.47215188658129|\n|  65.62923234827531|\n| 14.232465869832595|\n| 101.10206637405372|\n| -70.90405082350026|\n|  71.98242976633992|\n|-450.40314497328154|\n| 315.39172024014533|\n| 150.54924041607228|\n| -343.8099664802862|\n+-------------------+\n\nRMSE: 211.22957297313167\nr2: 0.7624201711079546\ntrainingSummary: org.apache.spark.ml.regression.LinearRegressionTrainingSummary = org.apache.spark.ml.regression.LinearRegressionTrainingSummary@6c4cbb05\n"},{"metadata":{},"data":{"text/html":""},"output_type":"execute_result","execution_count":66,"time":"Took: 1.161s, at 2022-02-24 10:44"}]},{"metadata":{"id":"BACEF07E15374A1C9593C55C854220C1"},"cell_type":"markdown","source":"### Conclusion\n\nWithout any optimization, the quality of the model is pretty good (r2 = 0.76). In reality, we can try to optimize this indicator by removing the anomalies, selecting the most important features to train model, adding more observations or more variables and fluctuating the parameters when we train model..."},{"metadata":{"id":"583B7F29D03B476C822E9CE82D032C97"},"cell_type":"markdown","source":"### Note :\n\nAll models created in Spark can be saved in HDFS by doing : \n\n* model.save(sc, \"file:///Apps/spark/data/mllib/testModelPath\") \n\nTo load it for future usage : \n\n* val sameModel = SVMModel.load(sc, \"file:///Apps/spark/data/mllib/testModelPath\"). \n\nIn this example, it's SVM model, so it's SVMModel.load\n\nPlus, for some models, we can convert it to PMML format. It's good if you knew already PMML, if not, it's also fine ;) you can read here : https://www.ibm.com/developerworks/library/ba-ind-PMML1/index.html.\n\nYou can see list of supported models in Spark here : https://spark.apache.org/docs/2.0.0-preview/mllib-pmml-model-export.html"},{"metadata":{"id":"B9009E5321B9489D9AF6EBF285798AD5"},"cell_type":"markdown","source":"# Exercice\n## Comment peut-on transformer ce code en créant un pipeline ? \n## Comment peut-on améliorer le modèle avec une cross-validation ?"},{"metadata":{"id":"C99F4B04F75D4DD19D5906AE82B726E4"},"cell_type":"markdown","source":"### Pre-processing\nPréparer les phases de `assembler` et on va ajouter l'étape de `Standardisation` des données.\nIl faut donc créer deux objets :\n- VectorAssembler\n- StandardScaler (https://spark.apache.org/docs/3.2.1/ml-features.html#standardscaler)\n\nRechercher dans la documentation les fonctions nécessaires : https://spark.apache.org/docs/3.2.1/ml-guide.html\n\nNous appelerons ces deux objets (ie. variables immuables \"val\") *assembler* et *scaler*. Ces deux premières étapes du pipeline, ont pour objectif de transformer et formater les données pour le modèle et de normaliser les données numériques, afin que les variables numériques soient comparables entre elles."},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":true,"id":"71AF0600FB4F4E08BB2E4F3BE8DB6B2A"},"cell_type":"code","source":["val assembler ="],"outputs":[]},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":true,"id":"23D7EDA9352C4E008CA339529C5F0245"},"cell_type":"code","source":["val scaler ="],"outputs":[]},{"metadata":{"id":"204798FD24C441E090025601A254B908"},"cell_type":"markdown","source":"### Model\nCréer le modèle de régression linéaire de votre choix (Linéaire simple, Lasso, Ridge, ElasticNet).\nCréer les ensembles de test et d'entraînement."},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":true,"id":"98AD35E33A524E18B7C1B09A029EEB22"},"cell_type":"code","source":["val lr ="],"outputs":[]},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":true,"id":"7498CA2A633C46668FA697D04F55CE13"},"cell_type":"code","source":["val train, test ="],"outputs":[]},{"metadata":{"id":"42002880A8A7461283EB1FC5112BC7B7"},"cell_type":"markdown","source":"### Pipeline\nCréer la chaîne pipeline avec les différentes étapes `stages`.\nPuis entraîner le modèle."},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":true,"id":"D53E0364265B45158667B09E840B9404"},"cell_type":"code","source":["val pipeline ="],"outputs":[]},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":true,"id":"BFBD9CEE8CAD45228788B4FACF76C0BD"},"cell_type":"code","source":["val lrModel ="],"outputs":[]},{"metadata":{"id":"E0121446C88D49E1980CB68F1C47F09E"},"cell_type":"markdown","source":"### Metrics\nCalculer les métriques des erreurs usuelles, qui sont le RMSE et le coefficient de détermination (noté r2).\nLe coefficient de détermination est une mesure de la qualité de la prédiction d'une régression linéaire. Il représente le pourcentage de la variance expliquée par la régression (ie. des prédictions faites) sur la variance totale de la variable réelle. Cet indicateur estime donc la corrélation entre les prédictions et la réalité. Plus il est proche de 1, plus le modèle est performat.\n\n**! Attention :**\nIci il ne suffit pas de faire `lrModel.summary` car ici `lrModel` est de type pipeline. Il faut donc extraire d'abord du pipeline la composante représentant le modèle.\nPour cela vous allez avoir besoin des objets suivants :\n- la fonction .stages(*numero_stage*) --> en spécifiant le numéro de l'étape qu'on souhaite extraire\n- la fonction .asInstanceOf[*type_stage*] --> en spécifiant le type de l'objet qu'on souhaite extraire du pipeline\n- la librairie du *type_stage* à extraire : import org.apache.spark.ml.regression.LinearRegressionModel\n"},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":true,"id":"8345B54E4DB649048CD04B87FA6E8D31"},"cell_type":"code","source":["val traininSummary ="],"outputs":[]},{"metadata":{"id":"A208CE7CF4314ABA974AD69B3EC5EF83"},"cell_type":"markdown","source":"### Test\nAppliquer le modèle entraîner sur l'ensemble de données de test. Vous allez donc devoir utiliser la fonction `.transform`, qui permet d'appliquer le modèle, avec les différentes étapes du pipeline qui sont nécessaires, automatiquement.\n\nEnsuite, évaluer les prédictions effectuées à l'aide de l'indicateur **RMSE**. Pour cela vous allez avoir besoin des objets suivants :\n- la librairie *org.apache.spark.ml*, contenant la fonction *evaluation.RegressionEvaluator* donc cela donne *org.apache.spark.ml.evaluation.RegressionEvaluator*\n- instancier un objet, qu'on appelera *evaluator*, permettant de calculer le RMSE entre la variable réelle `Water` (en tant que LabelCol) et la variable prédite `prediction` (en tant que Predictionol).\n\nSuivre la documentation suivante : https://spark.apache.org/docs/2.1.1/api/scala/index.html#org.apache.spark.ml.evaluation.RegressionEvaluator\n\nEnfin, appliquer l'objet *evaluator* créé, à l'objet *predictions* (dataframe contenant les prédictions faites sur l'ensemble de test), en utilisant la fonction *.evaluate*.\nPuis pour finir afficher le résultat du RMSE."},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":true,"id":"7D56760F543A4571802C81680C0FE6FC"},"cell_type":"code","source":["val predictions ="],"outputs":[]},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":true,"id":"DB541D5922BE4FC294A6B11CF86B05AD"},"cell_type":"code","source":["val evaluator ="],"outputs":[]},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":true,"id":"0FC79D65833448CBB956B2755FBB8995"},"cell_type":"code","source":["val rmse =\n","println(s\"Root Mean Squared Error (RMSE) on test data = $rmse\")"],"outputs":[]},{"metadata":{"id":"5F2C96BBF9C24976823C52C92AB37AFD"},"cell_type":"markdown","source":"### Cross validation\nL'objectif est d'utiliser la méthode de **Cross-validation** pour ajuster les différents paramètres de la régression linéaire.\nLes paramètres à ajuster sont les suivants :\n- regParam\n- elasticNetParam\n\nAfin de réaliser cela, nous allons tout d'abord créer une grille de recherche.\nPar exemple, voici l'ensemble des valeurs que nous allons tester pour chaque paramètre :\n- Pour `regParam` nous allos vouloir tester les valeurs suivantes : 0.1, 0.01, 0.2, 0.3\n- Pour `elasticNetParam` nous allos vouloir tester les valeurs suivantes : 0.1, 0.8\nUne fois que la grille est créée, il faut ensuite créer le modèle de cross-validation, puis pour terminer l'appliquer à l'ensemble d'entraînement.\n\nEn étudiant la documentation construisez les objets suivants :\n- paramGrid\n- cv\n- cvModel\n\nEnfin tester à nouveau le nouveau modèle `cvModel` sur l'ensemble de test."},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":true,"id":"25506317F1F64BD08A84E0220A60D957"},"cell_type":"code","source":["val paramGrid ="],"outputs":[]},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":true,"id":"4EFDB371C46341DB8295CACC193FDBDE"},"cell_type":"code","source":["val cv ="],"outputs":[]},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":true,"id":"AC216D92ECBB4B458D6CE0D724023F49"},"cell_type":"code","source":["val cvModel = "],"outputs":[]},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":true,"id":"0F564D0513CC4D8A82EC6A1AEFD8D4E9"},"cell_type":"code","source":["val predictionsCvModel ="],"outputs":[]},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":true,"id":"80810C1CC2674AFC80421A036E502708"},"cell_type":"code","source":["val evaluatorCvModel ="],"outputs":[]},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":true,"id":"E2BE352286384097829A406A35E0D8DB"},"cell_type":"code","source":["val rmse ="],"outputs":[]},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":true,"id":"B9D7210742184628A41B4D788FAEA7F5"},"cell_type":"markdown","source":"# Correction"},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":false,"id":"4B6F684853294C5697418F3E5C4D7FE6"},"cell_type":"code","source":["import org.apache.spark.sql.DataFrame\n","import org.apache.spark.ml.feature.VectorAssembler\n","import org.apache.spark.ml.feature.StandardScaler\n","import org.apache.spark.ml.regression.LinearRegression\n","import org.apache.spark.ml.{Pipeline, PipelineModel}\n","import org.apache.spark.ml.regression.LinearRegressionModel\n","\n","// instancier le vector\n","val assembler = new VectorAssembler() \n","                    .setInputCols(Array(\"Temperature\", \"Production\", \"Days\", \"Persons\")) \n","                    .setOutputCol(\"assembled_features\")\n","val scaler = new StandardScaler() \n","  .setInputCol(\"assembled_features\") \n","  .setOutputCol(\"features\") \n","  .setWithStd(true) \n","  .setWithMean(false)\n","\n","val lr = new LinearRegression() \n","  .setMaxIter(10) \n","  .setRegParam(0.3) \n","  .setElasticNetParam(0.8) \n","  .setLabelCol(\"Water\")\n","val Array(train, test) = data.randomSplit(Array(0.8, 0.2), seed = 11L)\n","\n","// on met les differentes etapes du pipeline\n","val pipeline = new Pipeline() \n","  .setStages(Array(assembler, scaler, lr))\n","val lrModel = pipeline.fit(train)\n","\n","// Summarize the model over the training set and print out some metrics\n","val trainingSummary = lrModel.stages(2).asInstanceOf[LinearRegressionModel].summary \n","println(s\"numIterations: ${trainingSummary.totalIterations}\")\n","println(s\"objectiveHistory: [${trainingSummary.objectiveHistory.mkString(\",\")}]\")\n","trainingSummary.residuals.show()\n","println(s\"RMSE: ${trainingSummary.rootMeanSquaredError}\")\n","println(s\"r2: ${trainingSummary.r2}\")"],"outputs":[{"name":"stdout","output_type":"stream","text":"numIterations: 11\nobjectiveHistory: [0.5,0.4628383730928235,0.2829957345590899,0.2543568688091208,0.29653911937073074,0.18868639292818604,0.16783880945958807,0.12182703917661503,0.11158188874690116,0.11052877064931642,0.10549294538646442]\n+-------------------+\n|          residuals|\n+-------------------+\n| 18.624537188468366|\n| 12.414151923516783|\n|  48.32058006373518|\n|   137.497254411021|\n|-39.917993971117085|\n| -48.89329149249215|\n| -411.0259980437604|\n|  185.1370180846202|\n|-101.66802303226223|\n|  160.0380896405577|\n|-110.47243822587825|\n| -269.7091729679487|\n| 419.65528642152367|\n+-------------------+\n\nRMSE: 201.14093389878042\nr2: 0.7935707380603677\nimport org.apache.spark.sql.DataFrame\nimport org.apache.spark.ml.feature.VectorAssembler\nimport org.apache.spark.ml.feature.StandardScaler\nimport org.apache.spark.ml.regression.LinearRegression\nimport org.apache.spark.ml.{Pipeline, PipelineModel}\nimport org.apache.spark.ml.regression.LinearRegressionModel\nassembler: org.apache.spark.ml.feature.VectorAssembler = vecAssembler_e0cac4e9cb59\nscaler: org.apache.spark.ml.feature.StandardScaler = stdScal_36e2b9bd2e53\nlr: org.apache.spark.ml.regression.LinearRegression = linReg_dc1bc67e1ccf\ntrain: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [Temperature: double, Production: int ... 3 more fields]\ntest: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [Temperature: double, Production: int ... 3 more fields]\npipeline: org.apac..."},{"metadata":{},"data":{"text/html":""},"output_type":"execute_result","execution_count":87,"time":"Took: 1.627s, at 2022-02-24 11:29"}]},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":false,"id":"0AA8089F9833441C9A02AA2EA483140E"},"cell_type":"code","source":["import org.apache.spark.ml.evaluation.RegressionEvaluator\n","\n","val predictions = lrModel.transform(test)\n","\n","// Select (prediction, true label) and compute test error. \n","val evaluator = new RegressionEvaluator() \n","  .setLabelCol(\"Water\") \n","  .setPredictionCol(\"prediction\") \n","  .setMetricName(\"rmse\")\n","\n","val rmse = evaluator.evaluate(predictions) \n","println(s\"Root Mean Squared Error (RMSE) on test data = $rmse\")"],"outputs":[{"name":"stdout","output_type":"stream","text":"Root Mean Squared Error (RMSE) on test data = 273.1306867994529\nimport org.apache.spark.ml.evaluation.RegressionEvaluator\npredictions: org.apache.spark.sql.DataFrame = [Temperature: double, Production: int ... 6 more fields]\nevaluator: org.apache.spark.ml.evaluation.RegressionEvaluator = regEval_ca1c8e73fc01\nrmse: Double = 273.1306867994529\n"},{"metadata":{},"data":{"text/html":""},"output_type":"execute_result","execution_count":89,"time":"Took: 1.081s, at 2022-02-24 11:30"}]},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":false,"id":"B0116EB0690C4C69A4B0869ED2B1D114"},"cell_type":"code","source":["import org.apache.spark.ml.tuning.{CrossValidator, ParamGridBuilder}\n","\n","// instancier le vector\n","val assembler = new VectorAssembler() \n","                    .setInputCols(Array(\"Temperature\", \"Production\", \"Days\", \"Persons\")) \n","                    .setOutputCol(\"assembled_features\")\n","val scaler = new StandardScaler() \n","  .setInputCol(\"assembled_features\") \n","  .setOutputCol(\"features\") \n","  .setWithStd(true) \n","  .setWithMean(false)\n","\n","val lr = new LinearRegression() \n","  .setMaxIter(10) \n","  .setRegParam(0.3) \n","  .setElasticNetParam(0.8) \n","  .setLabelCol(\"Water\")\n","val Array(train, test) = data.randomSplit(Array(0.8, 0.2), seed = 11L)\n","\n","// on met les differentes etapes du pipeline\n","val pipeline = new Pipeline() \n","  .setStages(Array(assembler, scaler, lr))\n","val lrModel = pipeline.fit(train)\n","\n","// on cree la grille des parametres qu'il va devoir tester\n","val paramGrid = new ParamGridBuilder() \n","  .addGrid(lr.regParam, Array(0.1, 0.01, 0.2, 0.3))\n","  .addGrid(lr.elasticNetParam, Array(0.1, 0.8)) \n","  .build()\n","val cv = new CrossValidator() \n","  .setEstimator(pipeline) \n","  .setEvaluator(evaluator) \n","  .setEstimatorParamMaps(paramGrid) \n","  .setNumFolds(3)  // Use 3+ in practice, but 6 is well\n","val cvModel = cv.fit(train)"],"outputs":[{"name":"stdout","output_type":"stream","text":"import org.apache.spark.ml.tuning.{CrossValidator, ParamGridBuilder}\nassembler: org.apache.spark.ml.feature.VectorAssembler = vecAssembler_0a246ac0fbea\nscaler: org.apache.spark.ml.feature.StandardScaler = stdScal_654560b4188d\nlr: org.apache.spark.ml.regression.LinearRegression = linReg_bfaf34af3926\ntrain: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [Temperature: double, Production: int ... 3 more fields]\ntest: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [Temperature: double, Production: int ... 3 more fields]\npipeline: org.apache.spark.ml.Pipeline = pipeline_beab22694683\nlrModel: org.apache.spark.ml.PipelineModel = pipeline_beab22694683\nparamGrid: Array[org.apache.spark.ml.param.ParamMap] =\nArray({\n\tlinReg_bfaf34af3926-elasticNetParam: 0.1,\n\tlinReg_bfaf34af3926..."},{"metadata":{},"data":{"text/html":""},"output_type":"execute_result","execution_count":98,"time":"Took: 7.688s, at 2022-02-24 11:51"}]},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":false,"id":"9911FADAB6CB4E669930F1390B154657"},"cell_type":"code","source":["// Make predictions on test documents. cvModel uses the best model found (lrModel). \n","\n","// afficher les predictions\n","val predictionsCvModel = cvModel.transform(test)\n","\n","// evaluation\n","val evaluatorCvModel = new RegressionEvaluator() \n","  .setLabelCol(\"Salary\")  \n","  .setPredictionCol(\"prediction\") \n","  .setMetricName(\"rmse\") \n","val rmse = evaluator.evaluate(predictionsCvModel) \n","println(s\"Root Mean Squared Error (RMSE) on test data = $rmse\")"],"outputs":[{"name":"stdout","output_type":"stream","text":"Root Mean Squared Error (RMSE) on test data = 249.8005072117436\npredictionsCvModel: org.apache.spark.sql.DataFrame = [Temperature: double, Production: int ... 6 more fields]\nevaluatorCvModel: org.apache.spark.ml.evaluation.RegressionEvaluator = regEval_d70de646ad94\nrmse: Double = 249.8005072117436\n"},{"metadata":{},"data":{"text/html":""},"output_type":"execute_result","execution_count":96,"time":"Took: 1.133s, at 2022-02-24 11:48"}]}]}