{"metadata":{"id":"ec55adfb-802f-4d27-8898-709d9ba64895","name":"Formation 5 - ML - Exercise TP","user_save_timestamp":"1970-01-01T00:00:00.000Z","auto_save_timestamp":"1970-01-01T00:00:00.000Z","language_info":{"name":"scala","file_extension":"scala","codemirror_mode":"text/x-scala"},"trusted":true,"sparkNotebook":null,"customLocalRepo":null,"customRepos":null,"customDeps":null,"customImports":null,"customArgs":null,"customSparkConf":null,"customVars":null},"cells":[{"metadata":{"id":"FB896527C0CB4BA4B417DB072C0EA871"},"cell_type":"markdown","source":"# Exercise time!\n\nSo, after all the lessons, it's your turn now to practice how to build a linear regression model in Spark!\n\n### Exercise 1 : \n\nIn HDFS of factory02 : \"hdfs://hupi-factory-02-01-01-01/user/hupi/dataset_torusVN/geoVN.json\", we have a small dataset (40 observations) of some characteristics of climate of some cities in Vietnam (source : https://www.meteoblue.com/en/weather/). \n\nThe goal will be to build a linear regression model that predicts the temperature daily mean thanks to 8 quantitatives variables.\n\nFirst analyse stasticals summary and correlation between variables.\nPerform detailed data analysis.\n\n\n### Exercise 2 : \n\nLet's try building a linear regression model that predicts the temperature daily mean thanks to 8 variables below : \n- High cloud cover daily mean\n- Low cloud cover daily mean\n- Mean Sea Level Pressure daily mean\n- Medium cloud cover daily mean\n- Relative humidity daily mean\n- Shortwave Radiation - backwards daily sum\n- Total Precipitation daily sum\n- Total cloud cover daily mean\n\nTo put it simply, we'll just care about these numerical variables; otherwise in this dataset, we can also inspect the relation between temperature daily mean and the city (categorical variable)."},{"metadata":{"id":"6AAD409CA5754CA88413BB3251A8735A"},"cell_type":"markdown","source":"## Be careful when you load the json file !\n\n### The result is strange! it's not what you are expecting, right? \n\nIt's because the format of json needed in Spark is 1 object per line, here we have 1 object with multi-lines, so this code won't work, we should do this way..\n\nHelp :*With Spark SQL each line must contain a separate, self-contained valid JSON otherwise the computation fails* => make research on that"},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":true,"id":"C4AAA4EB462544688431CA92C3CC317C"},"cell_type":"code","source":[""],"outputs":[]},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":true,"id":"C746AB25E0E54FB6942F0AB7EB73EEBC"},"cell_type":"code","source":[""],"outputs":[]},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":true,"id":"342A44E650FF41F39D3A684BA78C5CD7"},"cell_type":"code","source":[""],"outputs":[]},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":true,"id":"1BACE02304D241BB9C4977FF7E9DC375"},"cell_type":"code","source":[""],"outputs":[]},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":true,"id":"6D4DD5E91B5643C88EC3957411621C31"},"cell_type":"code","source":[""],"outputs":[]},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":true,"id":"29D45C134ACC42A68A164B4D5DB89A9D"},"cell_type":"code","source":[""],"outputs":[]}],"nbformat":4}