{
  "metadata" : {
    "id" : "a54185d4-71a6-43cf-8d06-8bc6fe67650e",
    "name" : "Formated 11 - Spark Streaming",
    "user_save_timestamp" : "1970-01-01T00:00:00.000Z",
    "auto_save_timestamp" : "1970-01-01T00:00:00.000Z",
    "language_info" : {
      "name" : "scala",
      "file_extension" : "scala",
      "codemirror_mode" : "text/x-scala"
    },
    "trusted" : true,
    "sparkNotebook" : null,
    "customLocalRepo" : null,
    "customRepos" : null,
    "customDeps" : [
      "org.apache.spark % spark-streaming-kafka-0-8_2.11 % 2.2.0"
    ],
    "customImports" : null,
    "customArgs" : null,
    "customSparkConf" : null,
    "customVars" : null
  },
  "cells" : [
    {
      "metadata" : {
        "id" : "A47A0BCADB5B468E8A3C71B74CB86E2A"
      },
      "cell_type" : "markdown",
      "source" : "### Spark Streaming\n\nSpark Streaming is an extension of the core Spark API that enables scalable, high-throughput, \nfault-tolerant stream processing of live data streams. Data can be ingested from many sources like \nKafka, Flume, Kinesis, or TCP sockets, and can be processed using complex algorithms expressed \nwith high-level functions like map, reduce, join and window. Finally, processed data can be \npushed out to filesystems, databases, and live dashboards. In fact, you can apply Sparkâ€™s machine learning and graph processing algorithms on data streams."
    },
    {
      "metadata" : {
        "trusted" : true,
        "input_collapsed" : false,
        "collapsed" : false,
        "id" : "A6BF204B8C744958A2F23828C3DF4E1C"
      },
      "cell_type" : "code",
      "source" : [
        ""
      ],
      "outputs" : [
        {
          "metadata" : { },
          "data" : {
            "text/html" : ""
          },
          "output_type" : "execute_result",
          "execution_count" : 1,
          "time" : "Took: 1.398s, at 2017-09-05 09:37"
        }
      ]
    },
    {
      "metadata" : {
        "id" : "C5CFCAAC9A21424A83EB2A2A45EE3BF2"
      },
      "cell_type" : "markdown",
      "source" : "First, we import the names of the Spark Streaming classes and some implicit conversions from StreamingContext into our environment in order to add useful methods to other classes we need (like DStream). StreamingContext is the main entry point for all streaming functionality. We create a local StreamingContext with two execution threads, and a batch interval of 1 second."
    },
    {
      "metadata" : {
        "trusted" : true,
        "input_collapsed" : false,
        "collapsed" : false,
        "id" : "C9BB1D25CB1946838F4A2067B4187257"
      },
      "cell_type" : "code",
      "source" : [
        "import org.apache.spark.streaming._\n",
        "import org.apache.spark.streaming.StreamingContext._\n",
        "import org.apache.spark.streaming.kafka._\n",
        "import org.apache.spark.SparkConf"
      ],
      "outputs" : [
        {
          "name" : "stdout",
          "output_type" : "stream",
          "text" : "import org.apache.spark.streaming._\nimport org.apache.spark.streaming.StreamingContext._\nimport org.apache.spark.streaming.kafka._\nimport org.apache.spark.SparkConf\n"
        },
        {
          "metadata" : { },
          "data" : {
            "text/html" : ""
          },
          "output_type" : "execute_result",
          "execution_count" : 2,
          "time" : "Took: 1.108s, at 2017-09-06 03:55"
        }
      ]
    },
    {
      "metadata" : {
        "trusted" : true,
        "input_collapsed" : false,
        "collapsed" : false,
        "id" : "A8C0CE835D564BF686EDBFB0F10CA91E"
      },
      "cell_type" : "code",
      "source" : [
        "//val sparkConf = new SparkConf().setAppName(\"DirectKafkaWordCount\")\n",
        "val sparkConf = new SparkConf().setAppName(\"Test Kafka2\") //sparkContext.getConf\n",
        "val ssc =  new StreamingContext(sparkContext, Seconds(10))\n",
        "ssc.checkpoint(\"checkpoint\")\n",
        "//val Array(zkQuorum, group, topics, numThreads) = args*/\n",
        "val zkQuorum = \"hupi-factory-02-02-05-01:2181\"\n",
        "val group = \"DEMO_HUPI_VINCENT\"\n",
        "val topics = \"factory02_test123\"\n",
        "val numThreads = \"1\""
      ],
      "outputs" : [
        {
          "name" : "stdout",
          "output_type" : "stream",
          "text" : "sparkConf: org.apache.spark.SparkConf = org.apache.spark.SparkConf@6589c25b\nssc: org.apache.spark.streaming.StreamingContext = org.apache.spark.streaming.StreamingContext@7b7b096\nzkQuorum: String = hupi-factory-02-02-05-01:2181\ngroup: String = DEMO_HUPI_VINCENT\ntopics: String = factory02_test123\nnumThreads: String = 1\n"
        },
        {
          "metadata" : { },
          "data" : {
            "text/html" : ""
          },
          "output_type" : "execute_result",
          "execution_count" : 3,
          "time" : "Took: 1.169s, at 2017-09-06 03:56"
        }
      ]
    },
    {
      "metadata" : {
        "trusted" : true,
        "input_collapsed" : false,
        "collapsed" : false,
        "id" : "2AEAA458A4714D84B3C683BCC2420DF3"
      },
      "cell_type" : "code",
      "source" : [
        "// Print What is read from Kafka code\n",
        "val topicMap = topics.split(\",\").map((_, numThreads.toInt)).toMap\n",
        "val streamdata = KafkaUtils.createStream(ssc, zkQuorum, group, topicMap)\n",
        "streamdata.foreachRDD {\n",
        "  rdd => {\n",
        "    val lines = rdd.map(_._2)\n",
        "    println(\"lines pushed are \" + lines.count)\n",
        "  }\n",
        "}"
      ],
      "outputs" : [
        {
          "name" : "stdout",
          "output_type" : "stream",
          "text" : "topicMap: scala.collection.immutable.Map[String,Int] = Map(factory02_test123 -> 1)\nstreamdata: org.apache.spark.streaming.dstream.ReceiverInputDStream[(String, String)] = org.apache.spark.streaming.kafka.KafkaInputDStream@6fd1ce57\n"
        },
        {
          "metadata" : { },
          "data" : {
            "text/html" : ""
          },
          "output_type" : "execute_result",
          "execution_count" : 4,
          "time" : "Took: 2.099s, at 2017-09-06 03:56"
        }
      ]
    },
    {
      "metadata" : {
        "trusted" : true,
        "input_collapsed" : false,
        "collapsed" : false,
        "id" : "C2384F65AA764F988E4E47C19BECB27B"
      },
      "cell_type" : "code",
      "source" : [
        "ssc.start()\n",
        "ssc.awaitTermination()"
      ],
      "outputs" : [
        {
          "name" : "stdout",
          "output_type" : "stream",
          "text" : "lines pushed are 0\nlines pushed are 0\nlines pushed are 0\nlines pushed are 0\nlines pushed are 5\nlines pushed are 0\nlines pushed are 0\nlines pushed are 0\n"
        }
      ]
    },
    {
      "metadata" : {
        "trusted" : true,
        "input_collapsed" : false,
        "collapsed" : true,
        "id" : "1982B9D867AF4342BFA2B3CA7FE0280C"
      },
      "cell_type" : "code",
      "source" : [
        ""
      ],
      "outputs" : [ ]
    }
  ],
  "nbformat" : 4
}