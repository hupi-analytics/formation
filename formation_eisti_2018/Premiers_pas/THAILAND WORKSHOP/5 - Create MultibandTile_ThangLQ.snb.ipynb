{
  "metadata" : {
    "id" : "38f65047-412c-4bde-b7bd-ae38d93358b2",
    "name" : "5 - Create MultibandTile_ThangLQ.snb.ipynb",
    "user_save_timestamp" : "2018-02-08T03:50:14.480Z",
    "auto_save_timestamp" : "1970-01-01T00:00:00.000Z",
    "language_info" : {
      "name" : "scala",
      "file_extension" : "scala",
      "codemirror_mode" : "text/x-scala"
    },
    "trusted" : true,
    "sparkNotebook" : null,
    "customLocalRepo" : null,
    "customRepos" : [
      "osgeo % default % http://download.osgeo.org/webdav/geotools/ % maven"
    ],
    "customDeps" : [
      "org.locationtech.geotrellis % geotrellis-spark_2.11 % 1.2.0",
      "- org.apache.hadoop % hadoop-client % _",
      "- org.apache.spark % spark-core_2.11 % _",
      "- org.apache.spark % spark-mllib_2.11 % _",
      "- org.apache.spark % spark-repl_2.11 % _",
      "- org.scala-lang % _ % _"
    ],
    "customImports" : null,
    "customArgs" : null,
    "customSparkConf" : {
      "spark.driver.maxResultSize" : "2G"
    },
    "customVars" : null
  },
  "cells" : [
    {
      "metadata" : {
        "id" : "521A6D78431147C092BFC5ABF7CC9BBF"
      },
      "cell_type" : "markdown",
      "source" : "# Import libraries "
    },
    {
      "metadata" : {
        "trusted" : true,
        "input_collapsed" : false,
        "collapsed" : false,
        "id" : "BEA974C4F20540298704EF6055B877D6"
      },
      "cell_type" : "code",
      "source" : [
        "import geotrellis.raster._\n",
        "import geotrellis.raster.vectorize._\n",
        "import geotrellis.raster.vectorize\n",
        "import geotrellis.raster.io.geotiff.GeoTiff\n",
        "\n",
        "import geotrellis.spark._\n",
        "import geotrellis.spark.io.hadoop\n",
        "import geotrellis.spark.io.hadoop._\n",
        "import geotrellis.spark.io.RasterReader\n",
        "import geotrellis.spark.tiling.FloatingLayoutScheme\n",
        "\n",
        "import geotrellis.vector._\n",
        "\n",
        "import org.apache.hadoop.fs.Path\n",
        "\n",
        "import org.apache.spark.SparkContext\n",
        "import org.apache.spark.rdd._\n",
        "import org.apache.spark.mllib.linalg.{Vector, Vectors}\n",
        "\n",
        "import java.net.URI\n",
        "import scala.math.BigDecimal.RoundingMode"
      ],
      "outputs" : [
        {
          "name" : "stdout",
          "output_type" : "stream",
          "text" : "import geotrellis.raster._\nimport geotrellis.raster.vectorize._\nimport geotrellis.raster.vectorize\nimport geotrellis.raster.io.geotiff.GeoTiff\nimport geotrellis.spark._\nimport geotrellis.spark.io.hadoop\nimport geotrellis.spark.io.hadoop._\nimport geotrellis.spark.io.RasterReader\nimport geotrellis.spark.tiling.FloatingLayoutScheme\nimport geotrellis.vector._\nimport org.apache.hadoop.fs.Path\nimport org.apache.spark.SparkContext\nimport org.apache.spark.rdd._\nimport org.apache.spark.mllib.linalg.{Vector, Vectors}\nimport java.net.URI\nimport scala.math.BigDecimal.RoundingMode\n"
        },
        {
          "metadata" : { },
          "data" : {
            "text/html" : ""
          },
          "output_type" : "execute_result",
          "execution_count" : 1,
          "time" : "Took: 1.800s, at 2018-03-11 03:46"
        }
      ]
    },
    {
      "metadata" : {
        "id" : "3295672897DE455D96B610F15D2F496F"
      },
      "cell_type" : "markdown",
      "source" : "# Important variables to make the library work"
    },
    {
      "metadata" : {
        "trusted" : true,
        "input_collapsed" : false,
        "collapsed" : false,
        "id" : "58AFFA1374F74CAF85AAC7C15674788A"
      },
      "cell_type" : "code",
      "source" : [
        "implicit val sparkContext = sc\n",
        "val rr = implicitly[RasterReader[HadoopGeoTiffRDD.Options, (ProjectedExtent, Tile)]]"
      ],
      "outputs" : [
        {
          "name" : "stdout",
          "output_type" : "stream",
          "text" : "sparkContext: org.apache.spark.SparkContext = org.apache.spark.SparkContext@20c5ad1c\nrr: geotrellis.spark.io.RasterReader[geotrellis.spark.io.hadoop.HadoopGeoTiffRDD.Options,(geotrellis.vector.ProjectedExtent, geotrellis.raster.Tile)] = geotrellis.spark.io.RasterReader$$anon$1@4424a9f4\n"
        },
        {
          "metadata" : { },
          "data" : {
            "text/html" : ""
          },
          "output_type" : "execute_result",
          "execution_count" : 2,
          "time" : "Took: 1.650s, at 2018-03-11 03:46"
        }
      ]
    },
    {
      "metadata" : {
        "trusted" : true,
        "input_collapsed" : false,
        "collapsed" : false,
        "id" : "ED0019344B01467A9C0D0048B5876E34"
      },
      "cell_type" : "markdown",
      "source" : "# Parameters"
    },
    {
      "metadata" : {
        "trusted" : true,
        "input_collapsed" : false,
        "collapsed" : false,
        "id" : "680684E89DBF4895816D257D144E031A"
      },
      "cell_type" : "code",
      "source" : [
        "val HdfsUrl = \"hdfs://hupi-factory-02-01-01-01/\"\n",
        "val dataRepo = \"user/factory02/thailand_workshop/data_usgs/\"\n",
        "val saveRepo = \"user/factory02/thailand_workshop/data_multiBands/\"\n",
        "val landsatName = \"LC08_L1TP_125052_20171231_20180103_01_T1\""
      ],
      "outputs" : [
        {
          "name" : "stdout",
          "output_type" : "stream",
          "text" : "HdfsUrl: String = hdfs://hupi-factory-02-01-01-01/\ndataRepo: String = user/factory02/thailand_workshop/data_usgs/\nsaveRepo: String = user/factory02/thailand_workshop/data_multiBands/\nlandsatName: String = LC08_L1TP_125052_20171231_20180103_01_T1\n"
        },
        {
          "metadata" : { },
          "data" : {
            "text/html" : ""
          },
          "output_type" : "execute_result",
          "execution_count" : 4,
          "time" : "Took: 1.117s, at 2018-03-11 03:47"
        }
      ]
    },
    {
      "metadata" : {
        "trusted" : true,
        "input_collapsed" : false,
        "collapsed" : false,
        "id" : "F601A4231BDE4324835CCB0D26C5020A"
      },
      "cell_type" : "code",
      "source" : [
        "val savePath = new Path(HdfsUrl + saveRepo + landsatName + \".tif\")\n",
        "val savePath_reduced = new Path(HdfsUrl + saveRepo + landsatName + \"_reduced.tif\")"
      ],
      "outputs" : [
        {
          "name" : "stdout",
          "output_type" : "stream",
          "text" : "savePath: org.apache.hadoop.fs.Path = hdfs://hupi-factory-02-01-01-01/user/factory02/thailand_workshop/data_multiBands/LC08_L1TP_125052_20171231_20180103_01_T1.tif\nsavePath_reduced: org.apache.hadoop.fs.Path = hdfs://hupi-factory-02-01-01-01/user/factory02/thailand_workshop/data_multiBands/LC08_L1TP_125052_20171231_20180103_01_T1_reduced.tif\n"
        },
        {
          "metadata" : { },
          "data" : {
            "text/html" : ""
          },
          "output_type" : "execute_result",
          "execution_count" : 5,
          "time" : "Took: 1.107s, at 2018-03-11 03:47"
        }
      ]
    },
    {
      "metadata" : {
        "id" : "EB94F7062DCD4381939E46F60EAC3EC7"
      },
      "cell_type" : "markdown",
      "source" : "# To avoid duplicates in HDFS"
    },
    {
      "metadata" : {
        "trusted" : true,
        "input_collapsed" : false,
        "collapsed" : false,
        "id" : "CFB61B1049804EF78E57A79A504E7010"
      },
      "cell_type" : "code",
      "source" : [
        "val conf = sc.hadoopConfiguration \n",
        "\n",
        "// delete the images if it existed already\n",
        "val fs = org.apache.hadoop.fs.FileSystem.get(new java.net.URI(HdfsUrl), conf)\n",
        "fs.delete(savePath,true)\n",
        "fs.delete(savePath_reduced,true)"
      ],
      "outputs" : [
        {
          "name" : "stdout",
          "output_type" : "stream",
          "text" : "conf: org.apache.hadoop.conf.Configuration = Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml\nfs: org.apache.hadoop.fs.FileSystem = DFS[DFSClient[clientName=DFSClient_NONMAPREDUCE_-1508474622_110, ugi=root (auth:SIMPLE)]]\nres6: Boolean = true\n"
        },
        {
          "metadata" : { },
          "data" : {
            "text/html" : "true"
          },
          "output_type" : "execute_result",
          "execution_count" : 6,
          "time" : "Took: 1.919s, at 2018-03-11 03:47"
        }
      ]
    },
    {
      "metadata" : {
        "trusted" : true,
        "input_collapsed" : false,
        "collapsed" : false,
        "id" : "1D0F6C59591946218F83B2433279742A"
      },
      "cell_type" : "code",
      "source" : [
        "val conf = sc.hadoopConfiguration \n",
        "\n",
        "// delete the images if it existed already\n",
        "val fs = org.apache.hadoop.fs.FileSystem.get(new java.net.URI(HdfsUrl), conf)\n",
        "fs.delete(new Path(HdfsUrl + saveRepo),true)"
      ],
      "outputs" : [
        {
          "name" : "stdout",
          "output_type" : "stream",
          "text" : "conf: org.apache.hadoop.conf.Configuration = Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml\nfs: org.apache.hadoop.fs.FileSystem = DFS[DFSClient[clientName=DFSClient_NONMAPREDUCE_-1508474622_110, ugi=root (auth:SIMPLE)]]\nres8: Boolean = true\n"
        },
        {
          "metadata" : { },
          "data" : {
            "text/html" : "true"
          },
          "output_type" : "execute_result",
          "execution_count" : 7,
          "time" : "Took: 1.415s, at 2018-03-11 03:47"
        }
      ]
    },
    {
      "metadata" : {
        "id" : "CCDED50A968747EC9129BFCD30776FD3"
      },
      "cell_type" : "markdown",
      "source" : "# Load GeoTiff from HDFS"
    },
    {
      "metadata" : {
        "trusted" : true,
        "input_collapsed" : false,
        "collapsed" : false,
        "id" : "22DFCEA5D3424132952FBF65ED65069E"
      },
      "cell_type" : "code",
      "source" : [
        "val options =\n",
        "HadoopGeoTiffRDD.Options(\n",
        "  numPartitions = Some(100)\n",
        ")\n",
        "\n",
        "type LandsatKey = (ProjectedExtent, URI, Int)\n",
        "// For each RDD, we're going to include more information in the key, including:\n",
        "// - the ProjectedExtent\n",
        "// - the URI = bandPath(band)\n",
        "// - the future band value\n",
        "def uriToKey(bandIndex: Int): (URI, ProjectedExtent) => LandsatKey =\n",
        "  { (uri, pe) =>\n",
        "    (pe, uri, bandIndex)\n",
        "  }"
      ],
      "outputs" : [
        {
          "name" : "stdout",
          "output_type" : "stream",
          "text" : "options: geotrellis.spark.io.hadoop.HadoopGeoTiffRDD.Options = Options(List(.tif, .TIF, .tiff, .TIFF),None,TIFFTAG_DATETIME,yyyy:MM:dd HH:mm:ss,Some(256),Some(100),Some(134217728),None)\ndefined type alias LandsatKey\nuriToKey: (bandIndex: Int)(java.net.URI, geotrellis.vector.ProjectedExtent) => LandsatKey\n"
        },
        {
          "metadata" : { },
          "data" : {
            "text/html" : ""
          },
          "output_type" : "execute_result",
          "execution_count" : 9,
          "time" : "Took: 1.351s, at 2018-03-11 03:47"
        }
      ]
    },
    {
      "metadata" : {
        "trusted" : true,
        "input_collapsed" : false,
        "collapsed" : false,
        "id" : "89BA4EE4528546CE9A251DE53377E8FD"
      },
      "cell_type" : "code",
      "source" : [
        "// Function to get GeoTiff for each bands\n",
        "def bandPath(b: String) = s\"hdfs://hupi-factory-02-01-01-01/user/factory02/thailand_workshop/data_usgs/${landsatName}/${landsatName}_${b}.TIF\""
      ],
      "outputs" : [
        {
          "name" : "stdout",
          "output_type" : "stream",
          "text" : "bandPath: (b: String)String\n"
        },
        {
          "metadata" : { },
          "data" : {
            "text/html" : ""
          },
          "output_type" : "execute_result",
          "execution_count" : 11,
          "time" : "Took: 0.975s, at 2018-03-11 03:47"
        }
      ]
    },
    {
      "metadata" : {
        "trusted" : true,
        "input_collapsed" : false,
        "collapsed" : false,
        "id" : "2C503226B90B4D3EBA42AB1909E9FBF7"
      },
      "cell_type" : "code",
      "source" : [
        "// List of all bands needed from HDFS (except band 8 : Panchromatic?, it size is different than others, \n",
        "// the resolution is 15m while others are 30m -> maybe we can't treat this band the same ways like others ?) \n",
        "val listBands = Array(\"B1\", \"B2\", \"B3\", \"B4\", \"B5\", \"B6\", \"B7\", \"B9\", \"B10\", \"B11\")"
      ],
      "outputs" : [
        {
          "name" : "stdout",
          "output_type" : "stream",
          "text" : "listBands: Array[String] = Array(B1, B2, B3, B4, B5, B6, B7, B9, B10, B11)\n"
        },
        {
          "metadata" : { },
          "data" : {
            "text/html" : ""
          },
          "output_type" : "execute_result",
          "execution_count" : 13,
          "time" : "Took: 0.958s, at 2018-03-11 03:47"
        }
      ]
    },
    {
      "metadata" : {
        "trusted" : true,
        "input_collapsed" : false,
        "collapsed" : false,
        "id" : "4FB37D13F5F24976B495CBA0B525969D"
      },
      "cell_type" : "code",
      "source" : [
        "// We initialize variable multiBands\n",
        "var multibands = HadoopGeoTiffRDD[ProjectedExtent, LandsatKey, Tile](\n",
        "      new Path(bandPath(\"B1\")), \n",
        "      uriToKey(0),\n",
        "      options)\n",
        "\n",
        "// We create a loop that do the union of all bands \n",
        "for (i <- 1 to listBands.length - 1) {\n",
        "  val band = listBands(i)\n",
        "  val bandTile = HadoopGeoTiffRDD[ProjectedExtent, LandsatKey, Tile](\n",
        "          new Path(bandPath(band)), \n",
        "          uriToKey(i),\n",
        "          options)\n",
        "  multibands = sc.union(multibands, bandTile) \n",
        "}"
      ],
      "outputs" : [
        {
          "name" : "stdout",
          "output_type" : "stream",
          "text" : "<console>:4: error: error writing object $eval: /tmp/spark-notebook-repl-078013b9-f994-45ff-9eaa-c1eccc840e8c/class: /tmp/spark-notebook-repl-078013b9-f994-45ff-9eaa-c1eccc840e8c/$line38 is not a directory\nobject $eval {\n       ^\norg.apache.spark.SparkException: Job aborted due to stage failure: Task 3 in stage 0.0 failed 1 times, most recent failure: Lost task 3.0 in stage 0.0 (TID 3, localhost, executor driver): org.apache.spark.SparkException: Process List(tar, -xzf, log4j-1.2.17.tar.gz) exited with code 2\n\tat org.apache.spark.util.Utils$.executeAndGetOutput(Utils.scala:1196)\n\tat org.apache.spark.util.Utils$.fetchFile(Utils.scala:469)\n\tat org.apache.spark.executor.Executor$$anonfun$org$apache$spark$executor$Executor$$updateDependencies$5.apply(Executor.scala:659)\n\tat org.apache.spark.executor.Executor$$anonfun$org$apache$spark$executor$Executor$$updateDependencies$5.apply(Executor.scala:651)\n\tat scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)\n\tat scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99)\n\tat scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99)\n\tat scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:230)\n\tat scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40)\n\tat scala.collection.mutable.HashMap.foreach(HashMap.scala:99)\n\tat scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)\n\tat org.apache.spark.executor.Executor.org$apache$spark$executor$Executor$$updateDependencies(Executor.scala:651)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:297)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n  at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1435)\n  at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1423)\n  at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1422)\n  at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n  at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n  at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1422)\n  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802)\n  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802)\n  at scala.Option.foreach(Option.scala:257)\n  at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:802)\n  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1650)\n  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1605)\n  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1594)\n  at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n  at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:628)\n  at org.apache.spark.SparkContext.runJob(SparkContext.scala:1925)\n  at org.apache.spark.SparkContext.runJob(SparkContext.scala:1938)\n  at org.apache.spark.SparkContext.runJob(SparkContext.scala:1951)\n  at org.apache.spark.SparkContext.runJob(SparkContext.scala:1965)\n  at org.apache.spark.rdd.RDD.count(RDD.scala:1158)\n  at geotrellis.spark.io.GeoTiffInfoReader$class.readWindows(GeoTiffInfoReader.scala:86)\n  at geotrellis.spark.io.hadoop.HadoopGeoTiffInfoReader.readWindows(HadoopGeoTiffInfoReader.scala:28)\n  at geotrellis.spark.io.hadoop.HadoopGeoTiffRDD$.apply(HadoopGeoTiffRDD.scala:122)\n  ... 75 elided\nCaused by: org.apache.spark.SparkException: Process List(tar, -xzf, log4j-1.2.17.tar.gz) exited with code 2\n  at org.apache.spark.util.Utils$.executeAndGetOutput(Utils.scala:1196)\n  at org.apache.spark.util.Utils$.fetchFile(Utils.scala:469)\n  at org.apache.spark.executor.Executor$$anonfun$org$apache$spark$executor$Executor$$updateDependencies$5.apply(Executor.scala:659)\n  at org.apache.spark.executor.Executor$$anonfun$org$apache$spark$executor$Executor$$updateDependencies$5.apply(Executor.scala:651)\n  at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)\n  at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99)\n  at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99)\n  at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:230)\n  at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40)\n  at scala.collection.mutable.HashMap.foreach(HashMap.scala:99)\n  at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)\n  at org.apache.spark.executor.Executor.org$apache$spark$executor$Executor$$updateDependencies(Executor.scala:651)\n  at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:297)\n  at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n  at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n  at java.lang.Thread.run(Thread.java:748)\n"
        }
      ]
    },
    {
      "metadata" : {
        "id" : "B3909C3E3C3B46F7B34FD26497003D6A"
      },
      "cell_type" : "markdown",
      "source" : "# Combine all SinglebandTiles into one MultibandTile"
    },
    {
      "metadata" : {
        "trusted" : true,
        "input_collapsed" : false,
        "collapsed" : false,
        "id" : "EEDDBBBB5CE942308578C7FE8B4C5CC7"
      },
      "cell_type" : "code",
      "source" : [
        "// After union these together, rearrange the elements so that we'll be able to group by key,\n",
        "// and the rearrange again to produce multiband tiles.\n",
        "val sourceTiles: RDD[(ProjectedExtent, MultibandTile)] = {\n",
        " multibands\n",
        "    .map { case ((pe, uri, bandIndex), tile) =>\n",
        "      // Get the center of the tile, which we will join on\n",
        "      val (x, y) = (pe.extent.center.x, pe.extent.center.y)\n",
        "\n",
        "      // Round the center coordinates in case there's any floating point errors\n",
        "      val center =\n",
        "        (\n",
        "          BigDecimal(x).setScale(5, RoundingMode.HALF_UP).doubleValue(),\n",
        "          BigDecimal(y).setScale(5, RoundingMode.HALF_UP).doubleValue()\n",
        "        )\n",
        "\n",
        "      // Get the scene ID from the path\n",
        "      val sceneId = uri.getPath.split('/').reverse.drop(1).head\n",
        "\n",
        "      val newKey = (sceneId, center)\n",
        "      val newValue = (pe, bandIndex, tile)\n",
        "      (newKey, newValue)\n",
        "    }\n",
        "    // we groupByKey to group all the values that have same LandsatName and same center (x, y)\n",
        "    .groupByKey()\n",
        "    // then, we should make sure that the order of bands in each Tile is the same\n",
        "    .map { case (oldKey, groupedValues) =>\n",
        "      val projectedExtent = groupedValues.head._1\n",
        "      val bands = Array.ofDim[Tile](groupedValues.size)\n",
        "      for((_, bandIndex, tile) <- groupedValues) {\n",
        "        bands(bandIndex) = tile\n",
        "      }\n",
        "      (projectedExtent, MultibandTile(bands))\n",
        "    }\n",
        "}"
      ],
      "outputs" : [
        {
          "name" : "stdout",
          "output_type" : "stream",
          "text" : "sourceTiles: org.apache.spark.rdd.RDD[(geotrellis.vector.ProjectedExtent, geotrellis.raster.MultibandTile)] = MapPartitionsRDD[110] at map at <console>:161\n"
        },
        {
          "metadata" : { },
          "data" : {
            "text/html" : ""
          },
          "output_type" : "execute_result",
          "execution_count" : 30,
          "time" : "Took: 1.539s, at 2018-01-18 13:12"
        }
      ]
    },
    {
      "metadata" : {
        "id" : "DDE6756C0D75461A8607B3D334516971"
      },
      "cell_type" : "markdown",
      "source" : "# Convert to a Raster\n\nTo be able to save in HDFS, we need to convert RDD[ProjectedExtent, Tile] to a Raster of MultibandTile"
    },
    {
      "metadata" : {
        "trusted" : true,
        "input_collapsed" : false,
        "collapsed" : false,
        "id" : "36394F2D0049471E8AD4B5D069E9852F"
      },
      "cell_type" : "code",
      "source" : [
        "val (_, metadata) = sourceTiles.collectMetadata[SpatialKey](FloatingLayoutScheme())\n",
        "val tiles = sourceTiles.tileToLayout[SpatialKey](metadata)"
      ],
      "outputs" : [
        {
          "name" : "stdout",
          "output_type" : "stream",
          "text" : "metadata: geotrellis.spark.TileLayerMetadata[geotrellis.spark.SpatialKey] = TileLayerMetadata(uint16raw,GridExtent(Extent(528585.0, 1156335.0, 758985.0, 1394415.0),30.0,30.0),Extent(528585.0, 1162785.0, 755415.0, 1394415.0),EPSG:32648,KeyBounds(SpatialKey(0,0),SpatialKey(29,30)))\ntiles: org.apache.spark.rdd.RDD[(geotrellis.spark.SpatialKey, geotrellis.raster.MultibandTile)] = ShuffledRDD[113] at reduceByKey at TileRDDMerge.scala:51\n"
        },
        {
          "metadata" : { },
          "data" : {
            "text/html" : ""
          },
          "output_type" : "execute_result",
          "execution_count" : 31,
          "time" : "Took: 28.091s, at 2018-01-18 13:13"
        }
      ]
    },
    {
      "metadata" : {
        "trusted" : true,
        "input_collapsed" : false,
        "collapsed" : false,
        "id" : "13B5CB5085314D17B73AFFA27A0F134A"
      },
      "cell_type" : "code",
      "source" : [
        "// Raster original without compression\n",
        "val raster = ContextRDD(tiles, metadata).stitch"
      ],
      "outputs" : [
        {
          "name" : "stdout",
          "output_type" : "stream",
          "text" : "raster: geotrellis.raster.Raster[geotrellis.raster.MultibandTile] = Raster(geotrellis.raster.ArrayMultibandTile@20e7f2dc,Extent(528585.0, 1156335.0, 758985.0, 1394415.0))\n"
        },
        {
          "metadata" : { },
          "data" : {
            "text/html" : ""
          },
          "output_type" : "execute_result",
          "execution_count" : 32,
          "time" : "Took: 18.271s, at 2018-01-18 13:13"
        }
      ]
    },
    {
      "metadata" : {
        "trusted" : true,
        "input_collapsed" : false,
        "collapsed" : false,
        "id" : "24D1D1CCDEF44F26815F19826F57609E"
      },
      "cell_type" : "code",
      "source" : [
        "// Raster with resolution reduced\n",
        "val raster_reduced =\n",
        "  ContextRDD(tiles, metadata)\n",
        "    .withContext { rdd =>\n",
        "      rdd.mapValues { tile =>\n",
        "        // Magic numbers! These were created by fiddling around with\n",
        "        // numbers until some example landsat images looked good enough\n",
        "        // to put on a map for some other project.\n",
        "        val (min, max) = (4000, 15176)\n",
        "        def clamp(z: Int) = {\n",
        "          if(isData(z)) { if(z > max) { max } else if(z < min) { min } else { z } }\n",
        "          else { z }\n",
        "        }\n",
        "        // we normalize the value of these rasters from min to 0, from max to 255 to reduce the resolution\n",
        "        val blue = tile.band(0).map(clamp _).delayedConversion(ByteCellType).normalize(min, max, 0, 255)\n",
        "        val green = tile.band(1).map(clamp _).delayedConversion(ByteCellType).normalize(min, max, 0, 255)\n",
        "        val red = tile.band(2).map(clamp _).delayedConversion(ByteCellType).normalize(min, max, 0, 255)\n",
        "        MultibandTile(blue, green, red)\n",
        "      }\n",
        "    }\n",
        "    .stitch"
      ],
      "outputs" : [
        {
          "name" : "stdout",
          "output_type" : "stream",
          "text" : "raster_reduced: geotrellis.raster.Raster[geotrellis.raster.MultibandTile] = Raster(geotrellis.raster.ArrayMultibandTile@719b826d,Extent(528585.0, 1156335.0, 758985.0, 1394415.0))\n"
        },
        {
          "metadata" : { },
          "data" : {
            "text/html" : ""
          },
          "output_type" : "execute_result",
          "execution_count" : 33,
          "time" : "Took: 3.598s, at 2018-01-18 13:13"
        }
      ]
    },
    {
      "metadata" : {
        "id" : "E66A5FFAE76A4ABE86C69E62A6E02865"
      },
      "cell_type" : "markdown",
      "source" : "### We can directly save to HDFS\n\nIf we don't reduce the size, we have to set the configuration \"spark.driver.maxResultSize\": \"2G\" in customSparkConf in Edit metadata notebook.\n\nFor Kmeans, we need the original one, without compression"
    },
    {
      "metadata" : {
        "trusted" : true,
        "input_collapsed" : false,
        "collapsed" : false,
        "id" : "E5B47BB114C8457883133EB2E3C9DAA8"
      },
      "cell_type" : "code",
      "source" : [
        "GeoTiff(raster, metadata.crs).write(savePath)"
      ],
      "outputs" : [
        {
          "name" : "stdout",
          "output_type" : "stream",
          "text" : "No codec found for hdfs://hupi-factory-02-01-01-01/user/factory02/thailand_workshop/data_multiBands/LC08_L1TP_125052_20171231_20180103_01_T1.tif, writing without compression.\n"
        },
        {
          "metadata" : { },
          "data" : {
            "text/html" : ""
          },
          "output_type" : "execute_result",
          "execution_count" : 34,
          "time" : "Took: 48.715s, at 2018-01-18 13:14"
        }
      ]
    },
    {
      "metadata" : {
        "id" : "87C84C0DA3A742B5806BBC08EAA24110"
      },
      "cell_type" : "markdown",
      "source" : "### Or save the reduced one"
    },
    {
      "metadata" : {
        "trusted" : true,
        "input_collapsed" : false,
        "collapsed" : false,
        "id" : "2F1D6E9B710A484BB43EEC82A4A096B5"
      },
      "cell_type" : "code",
      "source" : [
        "GeoTiff(raster_reduced, metadata.crs).write(savePath_reduced)"
      ],
      "outputs" : [
        {
          "name" : "stdout",
          "output_type" : "stream",
          "text" : "No codec found for hdfs://hupi-factory-02-01-01-01/user/factory02/thailand_workshop/data_multiBands/LC08_L1TP_125052_20171231_20180103_01_T1_reduced.tif, writing without compression.\n"
        },
        {
          "metadata" : { },
          "data" : {
            "text/html" : ""
          },
          "output_type" : "execute_result",
          "execution_count" : 35,
          "time" : "Took: 10.688s, at 2018-01-18 13:14"
        }
      ]
    }
  ],
  "nbformat" : 4
}