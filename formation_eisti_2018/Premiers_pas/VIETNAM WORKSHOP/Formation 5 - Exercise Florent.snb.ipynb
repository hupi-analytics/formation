{
  "metadata" : {
    "id" : "450e3c04-e45b-4e1a-8704-aba763fd08bd",
    "name" : "Formation 5 - Exercise Florent",
    "user_save_timestamp" : "1970-01-01T00:00:00.000Z",
    "auto_save_timestamp" : "1970-01-01T00:00:00.000Z",
    "language_info" : {
      "name" : "scala",
      "file_extension" : "scala",
      "codemirror_mode" : "text/x-scala"
    },
    "trusted" : true,
    "sparkNotebook" : null,
    "customLocalRepo" : null,
    "customRepos" : null,
    "customDeps" : null,
    "customImports" : null,
    "customArgs" : null,
    "customSparkConf" : null,
    "customVars" : null
  },
  "cells" : [
    {
      "metadata" : {
        "id" : "2021E860079A4BFA8136A6ED0A21F085"
      },
      "cell_type" : "markdown",
      "source" : "<img src=http://fd.perso.eisti.fr/Logos/TORUS2.png>"
    },
    {
      "metadata" : {
        "id" : "EBF9A2956F6E413680433166A00AB653"
      },
      "cell_type" : "markdown",
      "source" : "\n\n\n# Exercise time!\n\nSo, after all the lessons, it's your turn now to practice how to build a linear regression model in Spark!\n\n### Exercise 1 : \n\nIn HDFS of factory02 : \"hdfs://hupi-factory-02-01-01-01/user/hupi/dataset_torusVN/geoVN.json\", we have a small dataset (40 observations) of some characteristics of climate of some cities in Vietnam (source : https://www.meteoblue.com/en/weather/). \n\nLet's try building a linear regression model that predicts the temperature daily mean thanks to 8 variables below : \n- High cloud cover daily mean\n- Low cloud cover daily mean\n- Mean Sea Level Pressure daily mean\n- Medium cloud cover daily mean\n- Relative humidity daily mean\n- Shortwave Radiation - backwards daily sum\n- Total Precipitation daily sum\n- Total cloud cover daily mean\n\nTo put it simply, we'll just care about these numerical variables; otherwise in this dataset, we can also inspect the relation between temperature daily mean and the city (categorical variable).\n\n### Exercise 2 : \n\nCreate an API that can predict and give the result in the interface of Hupi. (suggestion : convert model to PMML format -> create an API in Hupi - Predict -> create a Widget...)\n\n### Exercise 3 :\n\nWith the dataset geoVN.json, let's create some visualizations in Hupi !"
    },
    {
      "metadata" : {
        "trusted" : true,
        "input_collapsed" : false,
        "collapsed" : false,
        "id" : "30AD90C695DC45788D886C7FE2BC1CA1"
      },
      "cell_type" : "code",
      "source" : [
        "// First initialize the system\n",
        "\n",
        "import org.apache.spark.sql.SparkSession\n",
        "\n",
        "val sparkSession = SparkSession\n",
        "  .builder()\n",
        "  .appName(\"linearRegModel_florent\")\n",
        "  .getOrCreate()\n",
        "\n",
        "// For implicit conversions like converting RDDs to DataFrames\n",
        "import sparkSession.implicits._"
      ],
      "outputs" : [
        {
          "name" : "stdout",
          "output_type" : "stream",
          "text" : "import org.apache.spark.sql.SparkSession\nsparkSession: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@5f8775d8\nimport sparkSession.implicits._\n"
        },
        {
          "metadata" : { },
          "data" : {
            "text/html" : ""
          },
          "output_type" : "execute_result",
          "execution_count" : 2,
          "time" : "Took: 1.827s, at 2017-09-06 09:49"
        }
      ]
    },
    {
      "metadata" : {
        "trusted" : true,
        "input_collapsed" : false,
        "collapsed" : false,
        "id" : "CA1BCFDB38A94CB8A790ABEEA9B4BE1E"
      },
      "cell_type" : "code",
      "source" : [
        "// Read the file\n",
        "val df = sparkSession.read.json(sparkContext.wholeTextFiles(\"hdfs://hupi-factory-02-01-01-01/user/hupi/dataset_torusVN/geoVN.json\").values)"
      ],
      "outputs" : [
        {
          "name" : "stdout",
          "output_type" : "stream",
          "text" : "df: org.apache.spark.sql.DataFrame = [City: string, Day: bigint ... 15 more fields]\n"
        },
        {
          "metadata" : { },
          "data" : {
            "text/html" : ""
          },
          "output_type" : "execute_result",
          "execution_count" : 3,
          "time" : "Took: 3.329s, at 2017-09-06 09:49"
        }
      ]
    },
    {
      "metadata" : {
        "trusted" : true,
        "input_collapsed" : false,
        "collapsed" : false,
        "id" : "549439E4A5A044C5A9CF9A96A0E2EA37"
      },
      "cell_type" : "code",
      "source" : [
        "// Statistics\n",
        "import org.apache.spark.mllib.linalg.Vectors\n",
        "import org.apache.spark.mllib.stat.{MultivariateStatisticalSummary, Statistics}"
      ],
      "outputs" : [
        {
          "name" : "stdout",
          "output_type" : "stream",
          "text" : "import org.apache.spark.mllib.linalg.Vectors\nimport org.apache.spark.mllib.stat.{MultivariateStatisticalSummary, Statistics}\n"
        },
        {
          "metadata" : { },
          "data" : {
            "text/html" : ""
          },
          "output_type" : "execute_result",
          "execution_count" : 4,
          "time" : "Took: 0.786s, at 2017-09-06 09:50"
        }
      ]
    },
    {
      "metadata" : {
        "trusted" : true,
        "input_collapsed" : false,
        "collapsed" : false,
        "id" : "6297D0E14A2C436AB4426F49F3DF8208"
      },
      "cell_type" : "code",
      "source" : [
        "// Convert df to RDD to be able to use the library MultiVariateStatisticalSummary. Here we choose only the necessary variables\n",
        "val rdd = df.drop(\"City\").drop(\"Day\").drop(\"Year\").drop(\"Month\").drop(\"timestamp\").drop(\"Lat\").drop(\"Lon\").drop(\"datestamp\")\n",
        "            .map(l => (l(6).asInstanceOf[Double], l(1).asInstanceOf[Double], l(2).asInstanceOf[Double],\n",
        "                       l(3).asInstanceOf[Double], l(4).asInstanceOf[Double], l(5).asInstanceOf[Double],\n",
        "                       l(0).asInstanceOf[Double], l(7).asInstanceOf[Double], l(8).asInstanceOf[Double])).rdd\n"
      ],
      "outputs" : [
        {
          "name" : "stdout",
          "output_type" : "stream",
          "text" : "rdd: org.apache.spark.rdd.RDD[(Double, Double, Double, Double, Double, Double, Double, Double, Double)] = MapPartitionsRDD[9] at rdd at <console>:87\n"
        },
        {
          "metadata" : { },
          "data" : {
            "text/html" : ""
          },
          "output_type" : "execute_result",
          "execution_count" : 5,
          "time" : "Took: 2.312s, at 2017-09-06 09:50"
        }
      ]
    },
    {
      "metadata" : {
        "trusted" : true,
        "input_collapsed" : false,
        "collapsed" : false,
        "id" : "00422B5EBFE3424B9A5DBB3FA8B542A3"
      },
      "cell_type" : "code",
      "source" : [
        "// Convert rdd to the rdd of vectors\n",
        "val observations = rdd.map(l => Vectors.dense(l._1, l._2, l._3, l._4, l._5, l._6, l._7, l._8, l._9))"
      ],
      "outputs" : [
        {
          "name" : "stdout",
          "output_type" : "stream",
          "text" : "observations: org.apache.spark.rdd.RDD[org.apache.spark.mllib.linalg.Vector] = MapPartitionsRDD[10] at map at <console>:86\n"
        },
        {
          "metadata" : { },
          "data" : {
            "text/html" : ""
          },
          "output_type" : "execute_result",
          "execution_count" : 6,
          "time" : "Took: 0.909s, at 2017-09-06 09:51"
        }
      ]
    },
    {
      "metadata" : {
        "trusted" : true,
        "input_collapsed" : false,
        "collapsed" : false,
        "id" : "B2567B039332482087CC3282B4BECCEF"
      },
      "cell_type" : "code",
      "source" : [
        "// Compute column summary statistics.\n",
        "val summary: MultivariateStatisticalSummary = Statistics.colStats(observations)\n"
      ],
      "outputs" : [
        {
          "name" : "stdout",
          "output_type" : "stream",
          "text" : "summary: org.apache.spark.mllib.stat.MultivariateStatisticalSummary = org.apache.spark.mllib.stat.MultivariateOnlineSummarizer@27969cb7\n"
        },
        {
          "metadata" : { },
          "data" : {
            "text/html" : ""
          },
          "output_type" : "execute_result",
          "execution_count" : 8,
          "time" : "Took: 0.798s, at 2017-09-06 09:51"
        }
      ]
    },
    {
      "metadata" : {
        "trusted" : true,
        "input_collapsed" : false,
        "collapsed" : false,
        "id" : "3990A5B67F4C4F9CBB802E8D18CF5349"
      },
      "cell_type" : "code",
      "source" : [
        "import org.apache.spark.mllib.linalg._\n",
        "import org.apache.spark.mllib.stat.Statistics\n",
        "import org.apache.spark.rdd.RDD"
      ],
      "outputs" : [
        {
          "name" : "stdout",
          "output_type" : "stream",
          "text" : "import org.apache.spark.mllib.linalg._\nimport org.apache.spark.mllib.stat.Statistics\nimport org.apache.spark.rdd.RDD\n"
        },
        {
          "metadata" : { },
          "data" : {
            "text/html" : ""
          },
          "output_type" : "execute_result",
          "execution_count" : 9,
          "time" : "Took: 0.747s, at 2017-09-06 09:51"
        }
      ]
    },
    {
      "metadata" : {
        "trusted" : true,
        "input_collapsed" : false,
        "collapsed" : false,
        "id" : "A91D07D26AE54FA3BEF0751B27B489A3"
      },
      "cell_type" : "code",
      "source" : [
        "// calculate the correlation matrix using Pearson's method. Use \"spearman\" for Spearman's method\n",
        "// If a method is not specified, Pearson's method will be used by default.\n",
        "val correlMatrix: Matrix = Statistics.corr(observations, \"pearson\")\n"
      ],
      "outputs" : [
        {
          "name" : "stdout",
          "output_type" : "stream",
          "text" : "correlMatrix: org.apache.spark.mllib.linalg.Matrix =\n1.0                   -0.6071271749991745  -0.7769139178723304    ... (9 total)\n-0.6071271749991745   1.0                  0.15213518644495438    ...\n-0.7769139178723304   0.15213518644495438  1.0                    ...\n-0.6317688824653139   0.5795801157147833   0.3987737633967863     ...\n-0.8783589380133455   0.7848585544175861   0.5355160905617118     ...\n0.7788360980837538    -0.7321669112163407  -0.5401236564390007    ...\n-0.07984925706043007  0.34275011990685916  -0.054919658480978115  ...\n-0.3896111580334033   0.6302167222500573   0.17145792931429643    ...\n-0.6190037706669987   0.6470804476922558   0.3584018763482733     ...\n"
        },
        {
          "metadata" : { },
          "data" : {
            "text/html" : ""
          },
          "output_type" : "execute_result",
          "execution_count" : 10,
          "time" : "Took: 1.636s, at 2017-09-06 09:51"
        }
      ]
    }
  ],
  "nbformat" : 4
}