{
  "metadata" : {
    "id" : "efe4c669-4d8c-4dc0-bd27-fa5f1f37f290",
    "name" : "Formation 4 - ML1 - Regression2",
    "user_save_timestamp" : "1970-01-01T00:00:00.000Z",
    "auto_save_timestamp" : "1970-01-01T00:00:00.000Z",
    "language_info" : {
      "name" : "scala",
      "file_extension" : "scala",
      "codemirror_mode" : "text/x-scala"
    },
    "trusted" : true,
    "sparkNotebook" : null,
    "customLocalRepo" : null,
    "customRepos" : null,
    "customDeps" : null,
    "customImports" : null,
    "customArgs" : null,
    "customSparkConf" : null,
    "customVars" : null
  },
  "cells" : [
    {
      "metadata" : {
        "trusted" : true,
        "input_collapsed" : false,
        "collapsed" : false,
        "id" : "130B34EFC82E4DD8883AD01DA7695EEC"
      },
      "cell_type" : "code",
      "source" : [
        "<img src=http://fd.perso.eisti.fr/Logos/TORUS2.png>"
      ],
      "outputs" : [
        {
          "name" : "stdout",
          "output_type" : "stream",
          "text" : "<console>:3: error: in XML literal: ' or \" delimited attribute value or '{' scala-expr '}' expected\n<img src=http://fd.perso.eisti.fr/Logos/TORUS2.png>\n         ^\n<console>:3: error: in XML literal: whitespace expected\n<img src=http://fd.perso.eisti.fr/Logos/TORUS2.png>\n          ^\n<console>:3: error: in XML literal: name cannot end in ':'\n<img src=http://fd.perso.eisti.fr/Logos/TORUS2.png>\n              ^\n<console>:3: error: in XML literal: '=' expected instead of '/'\n<img src=http://fd.perso.eisti.fr/Logos/TORUS2.png>\n               ^\n<console>:3: error: in XML literal: ' or \" delimited attribute value or '{' scala-expr '}' expected\n<img src=http://fd.perso.eisti.fr/Logos/TORUS2.png>\n                ^\n<console>:3: error: in XML literal: whitespace expected\n<img src=http://fd.perso.eisti.fr/Logos/TORUS2.png>\n                 ^\n<console>:3: error: in XML literal: '>' expected instead of '.'\n<img src=http://fd.perso.eisti.fr/Logos/TORUS2.png>\n                  ^\n"
        }
      ]
    },
    {
      "metadata" : {
        "id" : "5F687656ADF14F5B95AA9F6C8DF0035D"
      },
      "cell_type" : "markdown",
      "source" : "In this section, let us present to you some Machine Learning algorithms, there are many, but 3 algorithms below can be considered as the most popular in Machine Learning :\n\n- 1/ Regression - Linear Regression\n- 2/ Classification - Random Forest\n- 3/ Clustering - KMeans\n\nThis notebook will focus on the first one, we'll take a dataset and then build a linear regression model based on it. \n\n\"Linear regression is the most basic type of regression and commonly used predictive analysis.  The overall idea of regression is to examine two things: (1) does a set of predictor variables do a good job in predicting an outcome variable?  Is the model using the predictors accounting for the variability in the changes in the dependent variable? (2) Which variables in particular are significant predictors of the dependent variable?  And in what way do they--indicated by the magnitude and sign of the beta estimates--impact the dependent variable?  These regression estimates are used to explain the relationship between one dependent variable and one or more independent variables. (3) What is the regression equation that shows how the set of predictor variables can be used to predict the outcome?  The simplest form of the equation with one dependent and one independent variable is defined by the formula y = c + b*x, where y = estimated dependent score, c = constant, b = regression coefficients, and x = independent variable.\"\n\n(source : http://www.statisticssolutions.com/what-is-linear-regression/)"
    },
    {
      "metadata" : {
        "id" : "69E1DBB6815249F695137621B1D60C2B"
      },
      "cell_type" : "markdown",
      "source" : "### Read dataset (csv format) from HDFS\n\nHere we use the dataset from http://www.statsci.org/data/general/water.html \n\nThe target variable will be monthly water usage (gallons) and the variables descriptives are : \n- Average monthly temperature (F)\n- Amount of production (M pounds)\n- Number of plant operating days in the month\n- Number of persons on the monthly plant payroll"
    },
    {
      "metadata" : {
        "trusted" : true,
        "input_collapsed" : false,
        "collapsed" : false,
        "presentation" : {
          "tabs_state" : "{\n  \"tab_id\": \"#tab1599598013-0\"\n}",
          "pivot_chart_state" : "{\n  \"hiddenAttributes\": [],\n  \"menuLimit\": 200,\n  \"cols\": [],\n  \"rows\": [],\n  \"vals\": [],\n  \"exclusions\": {},\n  \"inclusions\": {},\n  \"unusedAttrsVertical\": 85,\n  \"autoSortUnusedAttrs\": false,\n  \"inclusionsInfo\": {},\n  \"aggregatorName\": \"Count\",\n  \"rendererName\": \"Table\"\n}"
        },
        "id" : "7D20B9182CD34E1887E1AC69C21C695D"
      },
      "cell_type" : "code",
      "source" : [
        "val sqlContext = new SQLContext(sc)\n",
        "\n",
        "val data = sqlContext.read.format(\"com.databricks.spark.csv\")\n",
        "              .option(\"header\", \"true\").option(\"inferSchema\", \"true\") \n",
        "              .load(\"hdfs://hupi-factory-02-01-01-01/user/hupi/dataset_torusVN/formation4_ML/water.csv\")"
      ],
      "outputs" : [
        {
          "name" : "stdout",
          "output_type" : "stream",
          "text" : "<console>:67: warning: constructor SQLContext in class SQLContext is deprecated: Use SparkSession.builder instead\n       val sqlContext = new SQLContext(sc)\n                        ^\nsqlContext: org.apache.spark.sql.SQLContext = org.apache.spark.sql.SQLContext@26fb35e6\ndata: org.apache.spark.sql.DataFrame = [Temperature: double, Production: int ... 3 more fields]\n"
        },
        {
          "metadata" : { },
          "data" : {
            "text/html" : ""
          },
          "output_type" : "execute_result",
          "execution_count" : 1,
          "time" : "Took: 3.884s, at 2017-08-30 08:23"
        }
      ]
    },
    {
      "metadata" : {
        "trusted" : true,
        "input_collapsed" : false,
        "collapsed" : false,
        "presentation" : {
          "tabs_state" : "{\n  \"tab_id\": \"#tab1154730564-0\"\n}",
          "pivot_chart_state" : "{\n  \"hiddenAttributes\": [],\n  \"menuLimit\": 200,\n  \"cols\": [],\n  \"rows\": [],\n  \"vals\": [],\n  \"exclusions\": {},\n  \"inclusions\": {},\n  \"unusedAttrsVertical\": 85,\n  \"autoSortUnusedAttrs\": false,\n  \"inclusionsInfo\": {},\n  \"aggregatorName\": \"Count\",\n  \"rendererName\": \"Table\"\n}"
        },
        "id" : "4350A09EB48640BA805CB74960005A05"
      },
      "cell_type" : "code",
      "source" : [
        "data.show()"
      ],
      "outputs" : [
        {
          "name" : "stdout",
          "output_type" : "stream",
          "text" : "+-----------+----------+----+-------+-----+\n|Temperature|Production|Days|Persons|Water|\n+-----------+----------+----+-------+-----+\n|       58.8|      7107|  21|    129| 3067|\n|       65.2|      6373|  22|    141| 2828|\n|       70.9|      6796|  22|    153| 2891|\n|       77.4|      9208|  20|    166| 2994|\n|       79.3|     14792|  25|    193| 3082|\n|       81.0|     14564|  23|    189| 3898|\n|       71.9|     11964|  20|    175| 3502|\n|       63.9|     13526|  23|    186| 3060|\n|       54.5|     12656|  20|    190| 3211|\n|       39.5|     14119|  20|    187| 3286|\n|       44.5|     16691|  22|    195| 3542|\n|       43.6|     14571|  19|    206| 3125|\n|       56.0|     13619|  22|    198| 3022|\n|       64.7|     14575|  22|    192| 2922|\n|       73.0|     14556|  21|    191| 3950|\n|       78.9|     18573|  21|    200| 4488|\n|       79.4|     15618|  22|    200| 3295|\n+-----------+----------+----+-------+-----+\n\n"
        },
        {
          "metadata" : { },
          "data" : {
            "text/html" : ""
          },
          "output_type" : "execute_result",
          "execution_count" : 2,
          "time" : "Took: 1.927s, at 2017-08-30 08:23"
        }
      ]
    },
    {
      "metadata" : {
        "trusted" : true,
        "input_collapsed" : false,
        "collapsed" : false,
        "id" : "3464411AEED342B1BA023CCE9FE89CBC"
      },
      "cell_type" : "code",
      "source" : [
        "data.printSchema()"
      ],
      "outputs" : [
        {
          "name" : "stdout",
          "output_type" : "stream",
          "text" : "root\n |-- Temperature: double (nullable = true)\n |-- Production: integer (nullable = true)\n |-- Days: integer (nullable = true)\n |-- Persons: integer (nullable = true)\n |-- Water: integer (nullable = true)\n\n"
        },
        {
          "metadata" : { },
          "data" : {
            "text/html" : ""
          },
          "output_type" : "execute_result",
          "execution_count" : 3,
          "time" : "Took: 1.073s, at 2017-08-30 08:23"
        }
      ]
    },
    {
      "metadata" : {
        "id" : "968621DB51114A018B9994922D915DCB"
      },
      "cell_type" : "markdown",
      "source" : "### Some descriptions of data"
    },
    {
      "metadata" : {
        "id" : "300A0B9D59314F1DAA271283F409F1AE"
      },
      "cell_type" : "markdown",
      "source" : "#### Statistics summary "
    },
    {
      "metadata" : {
        "trusted" : true,
        "input_collapsed" : false,
        "collapsed" : false,
        "id" : "C4857D0329C5474687D432EDAA83889F"
      },
      "cell_type" : "code",
      "source" : [
        "import org.apache.spark.mllib.linalg.Vectors\n",
        "import org.apache.spark.mllib.stat.{MultivariateStatisticalSummary, Statistics}"
      ],
      "outputs" : [
        {
          "name" : "stdout",
          "output_type" : "stream",
          "text" : "import org.apache.spark.mllib.linalg.Vectors\nimport org.apache.spark.mllib.stat.{MultivariateStatisticalSummary, Statistics}\n"
        },
        {
          "metadata" : { },
          "data" : {
            "text/html" : ""
          },
          "output_type" : "execute_result",
          "execution_count" : 5,
          "time" : "Took: 0.759s, at 2017-08-30 08:24"
        }
      ]
    },
    {
      "metadata" : {
        "trusted" : true,
        "input_collapsed" : false,
        "collapsed" : false,
        "id" : "E3956920DEFB424B8160B3E83D37B9E2"
      },
      "cell_type" : "code",
      "source" : [
        "// Convert df to RDD to be able to use the library MultiVariateStatisticalSummary.\n",
        "val rdd = data.map(l => (l(0).asInstanceOf[Double], l(1).asInstanceOf[Integer].toDouble, l(2).asInstanceOf[Integer].toDouble,\n",
        "                        l(3).asInstanceOf[Integer].toDouble, l(4).asInstanceOf[Integer].toDouble)).rdd"
      ],
      "outputs" : [
        {
          "name" : "stdout",
          "output_type" : "stream",
          "text" : "rdd: org.apache.spark.rdd.RDD[(Double, Double, Double, Double, Double)] = MapPartitionsRDD[14] at rdd at <console>:72\n"
        },
        {
          "metadata" : { },
          "data" : {
            "text/html" : ""
          },
          "output_type" : "execute_result",
          "execution_count" : 4,
          "time" : "Took: 1.630s, at 2017-08-30 08:24"
        }
      ]
    },
    {
      "metadata" : {
        "trusted" : true,
        "input_collapsed" : false,
        "collapsed" : false,
        "presentation" : {
          "tabs_state" : "{\n  \"tab_id\": \"#tab484739608-0\"\n}",
          "pivot_chart_state" : "{\n  \"hiddenAttributes\": [],\n  \"menuLimit\": 200,\n  \"cols\": [],\n  \"rows\": [],\n  \"vals\": [],\n  \"exclusions\": {},\n  \"inclusions\": {},\n  \"unusedAttrsVertical\": 85,\n  \"autoSortUnusedAttrs\": false,\n  \"inclusionsInfo\": {},\n  \"aggregatorName\": \"Count\",\n  \"rendererName\": \"Table\"\n}"
        },
        "id" : "00528782904447488936CBFDC883ECE7"
      },
      "cell_type" : "code",
      "source" : [
        "rdd.take(2)"
      ],
      "outputs" : [
        {
          "name" : "stdout",
          "output_type" : "stream",
          "text" : "res8: Array[(Double, Double, Double, Double, Double)] = Array((58.8,7107.0,21.0,129.0,3067.0), (65.2,6373.0,22.0,141.0,2828.0))\n"
        },
        {
          "metadata" : { },
          "data" : {
            "text/html" : "<div>\n      <script data-this=\"{&quot;dataId&quot;:&quot;anon15b3aeac801558136dba70bbc35bce55&quot;,&quot;dataInit&quot;:[],&quot;genId&quot;:&quot;484739608&quot;}\" type=\"text/x-scoped-javascript\">/*<![CDATA[*/req(['../javascripts/notebook/playground','../javascripts/notebook/magic/tabs'], \n      function(playground, _magictabs) {\n        // data ==> data-this (in observable.js's scopedEval) ==> this in JS => { dataId, dataInit, ... }\n        // this ==> scope (in observable.js's scopedEval) ==> this.parentElement ==> div.container below (toHtml)\n\n        playground.call(data,\n                        this\n                        ,\n                        {\n    \"f\": _magictabs,\n    \"o\": {}\n  }\n  \n                        \n                        \n                      );\n      }\n    );/*]]>*/</script>\n    <div>\n      <div>\n        <ul class=\"nav nav-tabs\" id=\"ul484739608\"><li>\n              <a href=\"#tab484739608-0\"><i class=\"fa fa-table\"/></a>\n            </li><li>\n              <a href=\"#tab484739608-1\"><i class=\"fa fa-cubes\"/></a>\n            </li></ul>\n\n        <div class=\"tab-content\" id=\"tab484739608\"><div class=\"tab-pane\" id=\"tab484739608-0\">\n            <div>\n      <script data-this=\"{&quot;dataId&quot;:&quot;anon7e231e413b3b0a9ebc8f96e808ebfefe&quot;,&quot;dataInit&quot;:[{&quot;_3&quot;:21,&quot;_2&quot;:7107,&quot;_1&quot;:58.8,&quot;_4&quot;:129,&quot;_5&quot;:3067},{&quot;_3&quot;:22,&quot;_2&quot;:6373,&quot;_1&quot;:65.2,&quot;_4&quot;:141,&quot;_5&quot;:2828}],&quot;genId&quot;:&quot;378684284&quot;}\" type=\"text/x-scoped-javascript\">/*<![CDATA[*/req(['../javascripts/notebook/playground','../javascripts/notebook/magic/tableChart'], \n      function(playground, _magictableChart) {\n        // data ==> data-this (in observable.js's scopedEval) ==> this in JS => { dataId, dataInit, ... }\n        // this ==> scope (in observable.js's scopedEval) ==> this.parentElement ==> div.container below (toHtml)\n\n        playground.call(data,\n                        this\n                        ,\n                        {\n    \"f\": _magictableChart,\n    \"o\": {\"headers\":[\"_1\",\"_2\",\"_3\",\"_4\",\"_5\"],\"width\":600,\"height\":400}\n  }\n  \n                        \n                        \n                      );\n      }\n    );/*]]>*/</script>\n    <div>\n      <span class=\"chart-total-item-count\"><p data-bind=\"text: value\"><script data-this=\"{&quot;valueId&quot;:&quot;anona3b675ce4cff32c6fdaa61a115cbb1e1&quot;,&quot;initialValue&quot;:&quot;2&quot;}\" type=\"text/x-scoped-javascript\">/*<![CDATA[*/\nreq(\n['observable', 'knockout'],\nfunction (O, ko) {\n  ko.applyBindings({\n      value: O.makeObservable(valueId, initialValue)\n    },\n    this\n  );\n});\n        /*]]>*/</script></p> entries total</span>\n      <span class=\"chart-sampling-warning\"><p data-bind=\"text: value\"><script data-this=\"{&quot;valueId&quot;:&quot;anon3f0b131f7c49cf8d75829aa16b3938e2&quot;,&quot;initialValue&quot;:&quot;&quot;}\" type=\"text/x-scoped-javascript\">/*<![CDATA[*/\nreq(\n['observable', 'knockout'],\nfunction (O, ko) {\n  ko.applyBindings({\n      value: O.makeObservable(valueId, initialValue)\n    },\n    this\n  );\n});\n        /*]]>*/</script></p></span>\n      <div>\n      </div>\n    </div></div>\n            </div><div class=\"tab-pane\" id=\"tab484739608-1\">\n            <div>\n      <script data-this=\"{&quot;dataId&quot;:&quot;anon98ccd2e70b4907ac3e3e3718e780cb65&quot;,&quot;dataInit&quot;:[{&quot;_3&quot;:21,&quot;_2&quot;:7107,&quot;_1&quot;:58.8,&quot;_4&quot;:129,&quot;_5&quot;:3067},{&quot;_3&quot;:22,&quot;_2&quot;:6373,&quot;_1&quot;:65.2,&quot;_4&quot;:141,&quot;_5&quot;:2828}],&quot;genId&quot;:&quot;1770021981&quot;}\" type=\"text/x-scoped-javascript\">/*<![CDATA[*/req(['../javascripts/notebook/playground','../javascripts/notebook/magic/pivotChart'], \n      function(playground, _magicpivotChart) {\n        // data ==> data-this (in observable.js's scopedEval) ==> this in JS => { dataId, dataInit, ... }\n        // this ==> scope (in observable.js's scopedEval) ==> this.parentElement ==> div.container below (toHtml)\n\n        playground.call(data,\n                        this\n                        ,\n                        {\n    \"f\": _magicpivotChart,\n    \"o\": {\"width\":600,\"height\":400,\"derivedAttributes\":{},\"extraOptions\":{}}\n  }\n  \n                        \n                        \n                      );\n      }\n    );/*]]>*/</script>\n    <div>\n      <span class=\"chart-total-item-count\"><p data-bind=\"text: value\"><script data-this=\"{&quot;valueId&quot;:&quot;anona6fe1af11cc1526f3c770785db79fdd1&quot;,&quot;initialValue&quot;:&quot;2&quot;}\" type=\"text/x-scoped-javascript\">/*<![CDATA[*/\nreq(\n['observable', 'knockout'],\nfunction (O, ko) {\n  ko.applyBindings({\n      value: O.makeObservable(valueId, initialValue)\n    },\n    this\n  );\n});\n        /*]]>*/</script></p> entries total</span>\n      <span class=\"chart-sampling-warning\"><p data-bind=\"text: value\"><script data-this=\"{&quot;valueId&quot;:&quot;anone90b82e15e68c947f479914eb10b6df4&quot;,&quot;initialValue&quot;:&quot;&quot;}\" type=\"text/x-scoped-javascript\">/*<![CDATA[*/\nreq(\n['observable', 'knockout'],\nfunction (O, ko) {\n  ko.applyBindings({\n      value: O.makeObservable(valueId, initialValue)\n    },\n    this\n  );\n});\n        /*]]>*/</script></p></span>\n      <div>\n      </div>\n    </div></div>\n            </div></div>\n      </div>\n    </div></div>"
          },
          "output_type" : "execute_result",
          "execution_count" : 6,
          "time" : "Took: 1.167s, at 2017-08-30 08:24"
        }
      ]
    },
    {
      "metadata" : {
        "trusted" : true,
        "input_collapsed" : false,
        "collapsed" : false,
        "id" : "CF81287EF5554647B4B1565D361E2A52"
      },
      "cell_type" : "code",
      "source" : [
        "// Convert rdd to the rdd of vectors\n",
        "val observations = rdd.map(l => Vectors.dense(l._1, l._2, l._3, l._4, l._5))"
      ],
      "outputs" : [
        {
          "name" : "stdout",
          "output_type" : "stream",
          "text" : "observations: org.apache.spark.rdd.RDD[org.apache.spark.mllib.linalg.Vector] = MapPartitionsRDD[15] at map at <console>:75\n"
        },
        {
          "metadata" : { },
          "data" : {
            "text/html" : ""
          },
          "output_type" : "execute_result",
          "execution_count" : 7,
          "time" : "Took: 1.034s, at 2017-08-30 08:24"
        }
      ]
    },
    {
      "metadata" : {
        "trusted" : true,
        "input_collapsed" : false,
        "collapsed" : false,
        "id" : "7F008D99219D49D983E6E2D8E396B785"
      },
      "cell_type" : "code",
      "source" : [
        "// Compute column summary statistics.\n",
        "val summary: MultivariateStatisticalSummary = Statistics.colStats(observations)\n",
        "println(\"Vectors of observations' mean : \" + summary.mean)  \n",
        "println(\"Vectors of observations' variance : \" + summary.variance)  \n",
        "println(\"Vectors of observations' number of column not null : \" + summary.numNonzeros)  \n",
        "println()"
      ],
      "outputs" : [
        {
          "name" : "stdout",
          "output_type" : "stream",
          "text" : "Vectors of observations' mean : [64.8529411764706,12900.470588235294,21.470588235294116,181.8235294117647,3303.705882352941]\nVectors of observations' variance : [182.52264705882362,1.2438223764705881E7,2.1397058823529416,483.7794117647057,199539.47058823524]\nVectors of observations' number of column not null : [17.0,17.0,17.0,17.0,17.0]\n\nsummary: org.apache.spark.mllib.stat.MultivariateStatisticalSummary = org.apache.spark.mllib.stat.MultivariateOnlineSummarizer@13e92eb0\n"
        },
        {
          "metadata" : { },
          "data" : {
            "text/html" : ""
          },
          "output_type" : "execute_result",
          "execution_count" : 8,
          "time" : "Took: 0.997s, at 2017-08-30 08:25"
        }
      ]
    },
    {
      "metadata" : {
        "id" : "2D5A83819A804487BEDDAEC5046EB347"
      },
      "cell_type" : "markdown",
      "source" : "#### Correlations of variables "
    },
    {
      "metadata" : {
        "trusted" : true,
        "input_collapsed" : false,
        "collapsed" : false,
        "id" : "D2CC7BB5AAED4C2F83373CCE36EB5FBB"
      },
      "cell_type" : "code",
      "source" : [
        "import org.apache.spark.mllib.linalg._\n",
        "import org.apache.spark.mllib.stat.Statistics\n",
        "import org.apache.spark.rdd.RDD"
      ],
      "outputs" : [
        {
          "name" : "stdout",
          "output_type" : "stream",
          "text" : "import org.apache.spark.mllib.linalg._\nimport org.apache.spark.mllib.stat.Statistics\nimport org.apache.spark.rdd.RDD\n"
        },
        {
          "metadata" : { },
          "data" : {
            "text/html" : ""
          },
          "output_type" : "execute_result",
          "execution_count" : 9,
          "time" : "Took: 0.642s, at 2017-08-30 08:25"
        }
      ]
    },
    {
      "metadata" : {
        "trusted" : true,
        "input_collapsed" : false,
        "collapsed" : false,
        "id" : "48C31D5093B44B0A9CE0F7B431C48CB0"
      },
      "cell_type" : "code",
      "source" : [
        "// calculate the correlation matrix using Pearson's method. Use \"spearman\" for Spearman's method\n",
        "// If a method is not specified, Pearson's method will be used by default.\n",
        "val correlMatrix: Matrix = Statistics.corr(observations, \"pearson\")\n",
        "println(correlMatrix.toString)"
      ],
      "outputs" : [
        {
          "name" : "stdout",
          "output_type" : "stream",
          "text" : "1.0                   -0.02410741870356305  0.43762975958335126   ... (5 total)\n-0.02410741870356305  1.0                   0.10573054707596519   ...\n0.43762975958335126   0.10573054707596519   1.0                   ...\n-0.08205777488270032  0.9184797375869633    0.03188119325449726   ...\n0.28575755805713965   0.6307494802500775    -0.08882582642644302  ...\ncorrelMatrix: org.apache.spark.mllib.linalg.Matrix =\n1.0                   -0.02410741870356305  0.43762975958335126   ... (5 total)\n-0.02410741870356305  1.0                   0.10573054707596519   ...\n0.43762975958335126   0.10573054707596519   1.0                   ...\n-0.08205777488270032  0.9184797375869633    0.03188119325449726   ...\n0.28575755805713965   0.6307494802500775    -0.08882582642644302  ...\n"
        },
        {
          "metadata" : { },
          "data" : {
            "text/html" : ""
          },
          "output_type" : "execute_result",
          "execution_count" : 10,
          "time" : "Took: 1.648s, at 2017-08-30 08:26"
        }
      ]
    },
    {
      "metadata" : {
        "id" : "7CB039FA56624D5A8B1CC54103094951"
      },
      "cell_type" : "markdown",
      "source" : "In this example, we don't have many variables descriptives, so we suppose that we can use all variables to build the regression model. Otherwise, we need to do a selection of variables to select the variables that affect the most the target variable. To do selection variable, depending on the type of variables, we can use different methods. In Spark, we have some basic tools to do that, for example https://spark.apache.org/docs/latest/ml-features.html#feature-selectors "
    },
    {
      "metadata" : {
        "id" : "9E0FE30DA33C419B8FD513684AA77990"
      },
      "cell_type" : "markdown",
      "source" : "###  Vector Assembler\n\nTo prepare for the construction of linear regression by using ML library, we have to have a data with 2 columns only (\"label\" and \"features\"). To have that, we need to put all the variables descriptives into a single vector column named \"features\" and column of the target variable should be renamed to \"label\". "
    },
    {
      "metadata" : {
        "trusted" : true,
        "input_collapsed" : false,
        "collapsed" : false,
        "id" : "C2A90C5B8CBE41D88975BFA9302B9544"
      },
      "cell_type" : "code",
      "source" : [
        "import org.apache.spark.ml.feature.VectorAssembler\n",
        "import org.apache.spark.ml.linalg.Vectors"
      ],
      "outputs" : [
        {
          "name" : "stdout",
          "output_type" : "stream",
          "text" : "import org.apache.spark.ml.feature.VectorAssembler\nimport org.apache.spark.ml.linalg.Vectors\n"
        },
        {
          "metadata" : { },
          "data" : {
            "text/html" : ""
          },
          "output_type" : "execute_result",
          "execution_count" : 3,
          "time" : "Took: 0.749s, at 2017-08-29 15:03"
        }
      ]
    },
    {
      "metadata" : {
        "trusted" : true,
        "input_collapsed" : false,
        "collapsed" : false,
        "id" : "36CC3C48CB6842358183FEA88AEF9760"
      },
      "cell_type" : "code",
      "source" : [
        "val assembler = new VectorAssembler()\n",
        "  .setInputCols(Array(\"Temperature\", \"Production\", \"Days\", \"Persons\"))\n",
        "  .setOutputCol(\"features\")"
      ],
      "outputs" : [
        {
          "name" : "stdout",
          "output_type" : "stream",
          "text" : "assembler: org.apache.spark.ml.feature.VectorAssembler = vecAssembler_8f5f61b63dfb\n"
        },
        {
          "metadata" : { },
          "data" : {
            "text/html" : ""
          },
          "output_type" : "execute_result",
          "execution_count" : 4,
          "time" : "Took: 0.862s, at 2017-08-29 15:03"
        }
      ]
    },
    {
      "metadata" : {
        "trusted" : true,
        "input_collapsed" : false,
        "collapsed" : false,
        "id" : "1121BE3C105944A09C40AC40C4D5A226"
      },
      "cell_type" : "code",
      "source" : [
        "val training = assembler.transform(data).select(\"Water\", \"features\").withColumnRenamed(\"Water\", \"label\")"
      ],
      "outputs" : [
        {
          "name" : "stdout",
          "output_type" : "stream",
          "text" : "training: org.apache.spark.sql.DataFrame = [label: int, features: vector]\n"
        },
        {
          "metadata" : { },
          "data" : {
            "text/html" : ""
          },
          "output_type" : "execute_result",
          "execution_count" : 1,
          "time" : "Took: 0.772s, at 2017-08-29 15:06"
        }
      ]
    },
    {
      "metadata" : {
        "id" : "394ED2D646624DA79B3936C16F94A809"
      },
      "cell_type" : "markdown",
      "source" : "### Build a linear regression model \n\nTo have the best model, we can try to fluctuate the parameters such as : number of max iterations, regularization parameters, etc. To find all the parameters supported by Spark that we can play with, you can see it in : https://spark.apache.org/docs/1.6.2/api/scala/index.html#org.apache.spark.ml.regression.LinearRegression"
    },
    {
      "metadata" : {
        "trusted" : true,
        "input_collapsed" : false,
        "collapsed" : false,
        "id" : "E305D86AF212465C88BDFF8B23175D95"
      },
      "cell_type" : "code",
      "source" : [
        "import org.apache.spark.ml.regression.LinearRegression\n",
        "\n",
        "val lr = new LinearRegression()\n",
        "  .setMaxIter(10)\n",
        "  .setRegParam(0.3)\n",
        "  .setElasticNetParam(0.8)\n",
        "\n",
        "// Fit the model\n",
        "val lrModel = lr.fit(training)"
      ],
      "outputs" : [
        {
          "name" : "stdout",
          "output_type" : "stream",
          "text" : "import org.apache.spark.ml.regression.LinearRegression\nlr: org.apache.spark.ml.regression.LinearRegression = linReg_e89726119d16\nlrModel: org.apache.spark.ml.regression.LinearRegressionModel = linReg_e89726119d16\n"
        },
        {
          "metadata" : { },
          "data" : {
            "text/html" : ""
          },
          "output_type" : "execute_result",
          "execution_count" : 7,
          "time" : "Took: 2.476s, at 2017-08-29 15:03"
        }
      ]
    },
    {
      "metadata" : {
        "trusted" : true,
        "input_collapsed" : false,
        "collapsed" : false,
        "id" : "B2C1EF47B7DD4BBBA36FDE5A4ED36BE6"
      },
      "cell_type" : "code",
      "source" : [
        "// Print the coefficients and intercept for linear regression\n",
        "println(s\"Coefficients: ${lrModel.coefficients} Intercept: ${lrModel.intercept}\")"
      ],
      "outputs" : [
        {
          "name" : "stdout",
          "output_type" : "stream",
          "text" : "Coefficients: [17.507462001951726,0.18981770762682473,-154.35737084062856,-18.869481872279586] Intercept: 6456.358526947011\n"
        },
        {
          "metadata" : { },
          "data" : {
            "text/html" : ""
          },
          "output_type" : "execute_result",
          "execution_count" : 8,
          "time" : "Took: 0.952s, at 2017-08-29 15:03"
        }
      ]
    },
    {
      "metadata" : {
        "id" : "5DAB8E2EDCFF4A80A8A4DDC0C03074A4"
      },
      "cell_type" : "markdown",
      "source" : "### Evaluation of model \n\nSome other metrics that can be computed : https://spark.apache.org/docs/1.6.2/api/scala/index.html#org.apache.spark.ml.regression.LinearRegressionTrainingSummary"
    },
    {
      "metadata" : {
        "trusted" : true,
        "input_collapsed" : false,
        "collapsed" : false,
        "id" : "64614EC412164C5191C232908B63E699"
      },
      "cell_type" : "code",
      "source" : [
        "// Summarize the model over the training set and print out some metrics\n",
        "val trainingSummary = lrModel.summary\n",
        "println(s\"numIterations: ${trainingSummary.totalIterations}\")\n",
        "println(s\"objectiveHistory: [${trainingSummary.objectiveHistory.mkString(\",\")}]\")\n",
        "trainingSummary.residuals.show()\n",
        "println(s\"RMSE: ${trainingSummary.rootMeanSquaredError}\")\n",
        "println(s\"r2: ${trainingSummary.r2}\")"
      ],
      "outputs" : [
        {
          "name" : "stdout",
          "output_type" : "stream",
          "text" : "numIterations: 11\nobjectiveHistory: [0.49999999999999645,0.46273434357816534,0.27509800560438624,0.24183329180895707,0.2829268336792645,0.17472246487386314,0.16061261622561143,0.13471854111964662,0.12675180848024353,0.1248114682474834,0.12084196355803098]\n+------------------+\n|         residuals|\n+------------------+\n|186.25416103531506|\n|-414.8817291627497|\n|-22.61650465369803|\n| -92.1637915883498|\n|-69.73952255210042|\n|-40.61437640103213|\n| 70.50085876721869|\n|-346.2654346679815|\n|213.72317955515246|\n| 375.5835602131224|\n|140.21959945509843|\n+------------------+\n\nRMSE: 224.33913605411445\nr2: 0.762244949839506\ntrainingSummary: org.apache.spark.ml.regression.LinearRegressionTrainingSummary = org.apache.spark.ml.regression.LinearRegressionTrainingSummary@67ac0fc3\n"
        },
        {
          "metadata" : { },
          "data" : {
            "text/html" : ""
          },
          "output_type" : "execute_result",
          "execution_count" : 9,
          "time" : "Took: 1.241s, at 2017-08-29 15:03"
        }
      ]
    },
    {
      "metadata" : {
        "id" : "BACEF07E15374A1C9593C55C854220C1"
      },
      "cell_type" : "markdown",
      "source" : "### Conclusion\n\nWithout any optimization, the quality of the model is pretty good (r2 = 0.76). In reality, we can try to optimize this indicator by removing the anomalies, selecting the most important features to train model, adding more observations or more variables and fluctuating the parameters when we train model..."
    },
    {
      "metadata" : {
        "id" : "583B7F29D03B476C822E9CE82D032C97"
      },
      "cell_type" : "markdown",
      "source" : "### Note :\n\nAll models created in Spark can be saved in HDFS by doing : \n\n* model.save(sc, \"file:///Apps/spark/data/mllib/testModelPath\") \n\nTo load it for future usage : \n\n* val sameModel = SVMModel.load(sc, \"file:///Apps/spark/data/mllib/testModelPath\"). \n\nIn this example, it's SVM model, so it's SVMModel.load\n\nPlus, for some models, we can convert it to PMML format. It's good if you knew already PMML, if not, it's also fine ;) you can read here : https://www.ibm.com/developerworks/library/ba-ind-PMML1/index.html.\n\nYou can see list of supported models in Spark here : https://spark.apache.org/docs/2.0.0-preview/mllib-pmml-model-export.html"
    },
    {
      "metadata" : {
        "trusted" : true,
        "input_collapsed" : false,
        "collapsed" : true,
        "id" : "B9D7210742184628A41B4D788FAEA7F5"
      },
      "cell_type" : "code",
      "source" : [
        ""
      ],
      "outputs" : [ ]
    }
  ]
}